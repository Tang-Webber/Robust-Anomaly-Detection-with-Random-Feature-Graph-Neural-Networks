{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "slbLTWGdYNan",
        "624ffRjLX2ZY",
        "RCEt9ee6YZQd",
        "FX6awK7DYUYO",
        "87QEmASwYmUT",
        "XaD6MtKrZYYe",
        "KIePvIj5Zmwk",
        "0tDK2c7oVBlZ",
        "2l9GxNi80-3y"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**LUNAR_Improvement**\n",
        "Files: https://drive.google.com/drive/folders/1SD1qmuH-5BdNVOEtOXN-gq3aXlTHYz7E?usp=drive_link\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2112.05355.pdf\n",
        "\n",
        "Code & Data: https://github.com/agoodge/LUNAR\n",
        "\n",
        "## Target\n",
        "\n",
        "1. decreasing performance by training on noisy data: add a proportion P% of anomalies into the training set (change P%=10%,30%,50%)\n",
        "\n",
        "2. decreasing performance by reduce the number of training data: reduce the number of normal samples in the training set (change to 50%, 70%, 90% of the original normal samples)\n",
        "\n",
        "3. random subsetting the features => K feature subsets cover all features (e.g., K=3,6,9), then construct K LUNAR models => average the scores of K models to be final outcome\n",
        "\n"
      ],
      "metadata": {
        "id": "_2hxIZtRarHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result"
      ],
      "metadata": {
        "id": "G-EdmcHTH_MY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Target 1 & 2\n"
      ],
      "metadata": {
        "id": "LXtRcQ-lH8rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=101bgQYb8hFgw-JWR4pbr9mj8FLVS2drt)\n",
        "\n"
      ],
      "metadata": {
        "id": "wiNGMTQ8Iuvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Target 3\n",
        "\n"
      ],
      "metadata": {
        "id": "qpbHI1KKIxrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1J9DrE2bOoFohEDLbeG0NXV13soYomocF)\n"
      ],
      "metadata": {
        "id": "XIQxv_apIzuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Experiment Records\n"
      ],
      "metadata": {
        "id": "Ndkgrp2cI4d7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Target 1 & 2\n",
        "\n",
        "  https://drive.google.com/file/d/1jtsUIPh61C0M4giwCyIUv_d9vdjbJzUO/view?usp=drive_link\n",
        "2. Target 3 - Testing Overlap percentage\n",
        "\n",
        "  https://drive.google.com/file/d/1m1LK0eslL-ngXRhGUK_ykyLVfOtGpSMC/view?usp=drive_link\n",
        "3. Target 3 - Summary\n",
        "\n",
        "  https://drive.google.com/file/d/1tkIBHew8PANg5l-7WubTQEM9PAzd6hHY/view?usp=drive_link"
      ],
      "metadata": {
        "id": "1oVey8X8IfRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Code"
      ],
      "metadata": {
        "id": "T8Gugx2THx7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 前置作業"
      ],
      "metadata": {
        "id": "slbLTWGdYNan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qnboMxJQr2Tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a47721c8-3960-443b-f0d3-219cb9e36ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUu0ba-HQ0k9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d113d2-976f-43ad-b1c4-1ed43576973a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "project_folder = '/content/drive/MyDrive/LUNAR'\n",
        "os.chdir(project_folder)\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0%2Bcu121.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0%2Bcu121.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries"
      ],
      "metadata": {
        "id": "624ffRjLX2ZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import faiss\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import MessagePassing"
      ],
      "metadata": {
        "id": "fzpCS99wUeBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 程式碼"
      ],
      "metadata": {
        "id": "RCEt9ee6YZQd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### variables.py"
      ],
      "metadata": {
        "id": "FX6awK7DYUYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model parameters for gnn\n",
        "n_epochs = 200\n",
        "lr = 0.001\n",
        "wd = 0.1\n",
        "\n",
        "# negative sample hyperparameters\n",
        "epsilon = 0.1\n",
        "proportion = 1"
      ],
      "metadata": {
        "id": "_pz6cKhYUZUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils.py"
      ],
      "metadata": {
        "id": "87QEmASwYmUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEGATIVE SAMPLE FUNCTIONS"
      ],
      "metadata": {
        "id": "Q4jHSTS_ZGe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def negative_samples(train_x, train_y, val_x, val_y, test_x, test_y, k, sample_type, proportion, epsilon):\n",
        "\n",
        "    # training set negative samples\n",
        "    neg_train_x, neg_train_y = generate_negative_samples(train_x, sample_type, proportion, epsilon)\n",
        "    # validation set negative samples\n",
        "    neg_val_x, neg_val_y = generate_negative_samples(val_x, sample_type, proportion, epsilon)\n",
        "\n",
        "    # concat data\n",
        "    x = np.vstack((train_x,neg_train_x,val_x,neg_val_x,test_x))\n",
        "    y = np.hstack((train_y,neg_train_y,val_y,neg_val_y,test_y))\n",
        "\n",
        "    # all training set\n",
        "    train_mask = np.hstack((np.ones(len(train_x)),np.ones(len(neg_train_x)),\n",
        "                            np.zeros(len(val_x)),np.zeros(len(neg_val_x)),\n",
        "                            np.zeros(len(test_x))))\n",
        "    # all validation set\n",
        "    val_mask = np.hstack((np.zeros(len(train_x)),np.zeros(len(neg_train_x)),\n",
        "                          np.ones(len(val_x)),np.ones(len(neg_val_x)),\n",
        "                          np.zeros(len(test_x))))\n",
        "    # all test set\n",
        "    test_mask = np.hstack((np.zeros(len(train_x)),np.zeros(len(neg_train_x)),\n",
        "                           np.zeros(len(val_x)),np.zeros(len(neg_val_x)),\n",
        "                           np.ones(len(test_x))))\n",
        "    # normal training points\n",
        "    neighbor_mask = np.hstack((np.ones(len(train_x)), np.zeros(len(neg_train_x)),\n",
        "                               np.zeros(len(val_y)), np.zeros(len(neg_val_x)),\n",
        "                               np.zeros(len(test_y))))\n",
        "\n",
        "    # find k nearest neighbours (idx) and their distances (dist) to each points in x within neighbour_mask==1\n",
        "    dist, idx = find_neighbors(x, y, neighbor_mask, k)\n",
        "\n",
        "    return x.astype('float32'), y.astype('float32'), neighbor_mask.astype('float32'), train_mask.astype('float32'), val_mask.astype('float32'), test_mask.astype('float32'), dist, idx\n",
        "\n",
        "# loading negative samples\n",
        "def generate_negative_samples(x, sample_type, proportion, epsilon):\n",
        "\n",
        "    n_samples = int(proportion*(len(x)))\n",
        "    n_dim = x.shape[-1]\n",
        "\n",
        "    #M\n",
        "    randmat = np.random.rand(n_samples,n_dim) < 0.3\n",
        "    # uniform samples\n",
        "    rand_unif = (epsilon* (1-2*np.random.rand(n_samples,n_dim)))\n",
        "    #  subspace perturbation samples\n",
        "    rand_sub = np.tile(x, (proportion,1)) + randmat*(epsilon*np.random.randn(n_samples,n_dim))\n",
        "\n",
        "    if sample_type == 'UNIFORM':\n",
        "        neg_x = rand_unif\n",
        "    if sample_type == 'SUBSPACE':\n",
        "        neg_x = rand_sub\n",
        "    if sample_type == 'MIXED':\n",
        "        # randomly sample from uniform and gaussian negative samples\n",
        "        neg_x = np.concatenate((rand_unif, rand_sub),0)\n",
        "        neg_x = neg_x[np.random.choice(np.arange(len(neg_x)), size = n_samples)]\n",
        "\n",
        "    neg_y = np.ones(len(neg_x))\n",
        "\n",
        "    return neg_x.astype('float32'), neg_y.astype('float32')"
      ],
      "metadata": {
        "id": "xjtkbhBtYwI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAPH FUNCTIONS"
      ],
      "metadata": {
        "id": "Iayk7CJxZTmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the k nearest neighbours of all x points out of the neighbour candidates\n",
        "def find_neighbors(x, y, neighbor_mask, k):\n",
        "\n",
        "    # nearest neighbour object\n",
        "    index = faiss.IndexFlatL2(x.shape[-1])\n",
        "    # add nearest neighbour candidates\n",
        "    index.add(x[neighbor_mask==1])\n",
        "\n",
        "    # distances and idx of neighbour points for the neighbour candidates (k+1 as the first one will be the point itself)\n",
        "    dist_train, idx_train = index.search(x[neighbor_mask==1], k = k+1)\n",
        "    # remove 1st nearest neighbours to remove self loops\n",
        "    dist_train, idx_train = dist_train[:,1:], idx_train[:,1:]\n",
        "    # distances and idx of neighbour points for the non-neighbour candidates\n",
        "    dist_test, idx_test = index.search(x[neighbor_mask==0], k = k)\n",
        "    #concat\n",
        "    dist = np.vstack((dist_train, dist_test))\n",
        "    idx = np.vstack((idx_train, idx_test))\n",
        "\n",
        "    return dist, idx\n",
        "\n",
        "# create graph object out of x, y, distances and indices of neighbours\n",
        "def build_graph(x, y, dist, idx):\n",
        "\n",
        "    # array like [0,0,0,0,0,1,1,1,1,1,...,n,n,n,n,n] for k = 5 (i.e. edges sources)\n",
        "    idx_source = np.repeat(np.arange(len(x)),dist.shape[-1]).astype('int32')\n",
        "    idx_source = np.expand_dims(idx_source,axis=0)\n",
        "\n",
        "    # edge targets, i.e. the nearest k neighbours of point 0, 1,..., n\n",
        "    idx_target = idx.flatten()\n",
        "    idx_target = np.expand_dims(idx_target,axis=0).astype('int32')\n",
        "\n",
        "    #stack source and target indices\n",
        "    idx = np.vstack((idx_source, idx_target))\n",
        "\n",
        "    # edge weights\n",
        "    attr = dist.flatten()\n",
        "    attr = np.sqrt(attr)\n",
        "    attr = np.expand_dims(attr, axis=1)\n",
        "\n",
        "    # into tensors\n",
        "    x = torch.tensor(x, dtype = torch.float32)\n",
        "    y = torch.tensor(y,dtype = torch.float32)\n",
        "    idx = torch.tensor(idx, dtype = torch.long)\n",
        "    attr = torch.tensor(attr, dtype = torch.float32)\n",
        "\n",
        "    #build PyTorch geometric Data object\n",
        "    data = Data(x = x, edge_index = idx, edge_attr = attr, y = y)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "bqwfA3ZRY3Eg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATASET FUNCTIONS"
      ],
      "metadata": {
        "id": "Cm41pHtcZWGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split training data into train set and validation set\n",
        "def split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
        "    val_mask = np.zeros(len(all_train_x))\n",
        "    val_mask[val_idx] = 1\n",
        "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
        "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x[train_y == 0])\n",
        "    train_x = scaler.transform(train_x)\n",
        "    val_x = scaler.transform(val_x)\n",
        "\n",
        "    if all_test_x is None:\n",
        "        test_x = val_x\n",
        "        test_y = val_y\n",
        "\n",
        "    test_x = scaler.transform(all_test_x)\n",
        "    test_y = all_test_y\n",
        "\n",
        "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
        "\n",
        "\n",
        "#load data\n",
        "def load_dataset(dataset,seed):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if dataset == 'MI-V':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['06','08','09','10']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = np.zeros(len(train_x))\n",
        "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset == 'MI-F':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['04', '05', '07', '16']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = np.zeros(len(train_x))\n",
        "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
        "        if dataset == 'SHUTTLE':\n",
        "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
        "        elif dataset == 'OPTDIGITS':\n",
        "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
        "        elif dataset == 'PENDIGITS':\n",
        "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset in ['THYROID','HRSS']:\n",
        "        if dataset == 'THYROID':\n",
        "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
        "        if dataset == 'HRSS':\n",
        "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
        "        label = data[:,-1].astype('float32').squeeze()\n",
        "        data = data[:,:-1].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset == 'SATELLITE':\n",
        "        data = loadmat('data/SATELLITE/satellite.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data = data[label == 0]\n",
        "        normal_label = label[label ==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "        test_x = normal_data[test_idx]\n",
        "        test_y = normal_label[test_idx]\n",
        "        test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "        test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
        "        test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y"
      ],
      "metadata": {
        "id": "hCR2GGmmUwoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lunar.py"
      ],
      "metadata": {
        "id": "XaD6MtKrZYYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Message passing scheme\n",
        "class GNN1(MessagePassing):\n",
        "    def __init__(self,k):\n",
        "        super(GNN1, self).__init__(flow=\"target_to_source\")\n",
        "        self.k = k\n",
        "        self.hidden_size = 256\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(k,self.hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden_size,self.hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden_size,self.hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden_size,1),\n",
        "            nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        self.network = self.network.to(dtype = torch.float32)\n",
        "        out = self.propagate(edge_index = edge_index, x=x, edge_attr=edge_attr, k = self.k, network=self.network)\n",
        "        return out\n",
        "\n",
        "    def message(self,x_i,x_j,edge_attr):\n",
        "        # message is the edge weight\n",
        "        return edge_attr\n",
        "\n",
        "    def aggregate(self, inputs, index, k, network):\n",
        "        # concatenate all k messages\n",
        "        self.input_aggr = inputs.reshape(-1,k)\n",
        "        # pass through network\n",
        "        out = self.network(self.input_aggr)\n",
        "        return out\n",
        "\n",
        "# GNN\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(GNN, self).__init__()\n",
        "        self.k = k\n",
        "        self.L1 = GNN1(self.k)\n",
        "    def forward(self,data):\n",
        "        self.edge_attr = data.edge_attr\n",
        "        self.edge_index = data.edge_index\n",
        "        self.x = data.x\n",
        "        out = self.L1(self.x, self.edge_index, self.edge_attr)\n",
        "        out = torch.squeeze(out,1)\n",
        "        return out\n",
        "\n",
        "def run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model):\n",
        "\n",
        "    # loss function\n",
        "    criterion = nn.MSELoss(reduction = 'none')\n",
        "\n",
        "    # path to save model parameters\n",
        "    model_path = 'saved_models/%s/%d/net_%d.pth' %(dataset,k,seed)\n",
        "    if not os.path.exists(os.path.dirname(model_path)):\n",
        "       os.makedirs(os.path.dirname(model_path))\n",
        "\n",
        "    x, y, neighbor_mask, train_mask, val_mask, test_mask, dist, idx = negative_samples(train_x, train_y, val_x, val_y, test_x, test_y, k, samples, proportion, epsilon)\n",
        "    data = build_graph(x, y, dist, idx)\n",
        "\n",
        "    data = data.to(device)\n",
        "    torch.manual_seed(seed)\n",
        "    net = GNN(k).to(device)\n",
        "\n",
        "    if train_new_model == True:\n",
        "\n",
        "        optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay = wd)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            net.eval()\n",
        "            out = net(data)\n",
        "            loss = criterion(out,data.y)\n",
        "\n",
        "            val_loss = loss[val_mask == 1].mean()\n",
        "            val_score = roc_auc_score(data.y[val_mask==1].cpu(),out[val_mask==1].cpu())\n",
        "\n",
        "            best_val_score = 0\n",
        "\n",
        "        # training\n",
        "        for epoch in range(n_epochs):\n",
        "            net.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = net(data)\n",
        "            # loss for training data only\n",
        "            loss = criterion(out[train_mask == 1],data.y[train_mask == 1]).sum()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                net.eval()\n",
        "                out = net(data)\n",
        "                loss = criterion(out,data.y)\n",
        "\n",
        "                val_loss = loss[val_mask == 1].mean()\n",
        "                val_score = roc_auc_score(data.y[val_mask==1].cpu(),out[val_mask==1].cpu())\n",
        "\n",
        "                # if new model gives the best validation set score\n",
        "                if val_score >= best_val_score:\n",
        "\n",
        "                    # save model parameters\n",
        "                    best_dict = {'epoch': epoch,\n",
        "                           'model_state_dict': deepcopy(net.state_dict()),\n",
        "                           'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
        "                           'val_loss': val_loss,\n",
        "                           'val_score': val_score,\n",
        "                           'k': k,}\n",
        "\n",
        "                    # save best model\n",
        "                    #torch.save(best_dict, model_path)\n",
        "\n",
        "                    # reset best score so far\n",
        "                    best_val_score = val_score\n",
        "\n",
        "        # load best model\n",
        "        net.load_state_dict(best_dict['model_state_dict'])\n",
        "\n",
        "    # if not training a new model, load the saved model\n",
        "    if train_new_model == False:\n",
        "\n",
        "        load_dict = torch.load(model_path)\n",
        "        net.load_state_dict(load_dict['model_state_dict'])\n",
        "\n",
        "    # testing\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        out = net(data)\n",
        "        loss = criterion(out,data.y)\n",
        "\n",
        "    # return output for test points\n",
        "    return out[test_mask==1].cpu()"
      ],
      "metadata": {
        "id": "hQVfnAaBU3ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### main.py"
      ],
      "metadata": {
        "id": "KIePvIj5Zmwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(args):\n",
        "    dataset = args.dataset\n",
        "    samples = args.samples\n",
        "    k = args.k\n",
        "    train_new_model = args.train_new_model\n",
        "    mode = args.mode\n",
        "\n",
        "    if mode == 0:\n",
        "        for seed in [0,4]:\n",
        "            #print(\"Running trial with random seed = %d\" %seed)\n",
        "            train_x, train_y, val_x, val_y, test_x, test_y = load_dataset(args.dataset,seed)\n",
        "            start = time.time()\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,args.dataset,seed,args.k,args.samples,args.train_new_model)\n",
        "            end = time.time()\n",
        "            score = 100*roc_auc_score(test_y, test_out)\n",
        "            print('Dataset: %s \\t Samples: %s \\t k: %d \\t Score: %.4f \\t Runtime: %.2f seconds' %(args.dataset,args.samples,args.k,score,(end-start)))\n",
        "    elif mode == 1:\n",
        "        Ps = [10, 30, 50]\n",
        "        for P in Ps:\n",
        "            print(f\"Percentage = {P} : \")\n",
        "            seed = 195\n",
        "            #print(\"Running trial with random seed = %d\" %seed)\n",
        "            train_x, train_y, val_x, val_y, test_x, test_y = target1_load_dataset(args.dataset,seed, P)\n",
        "            start = time.time()\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,args.dataset,seed,args.k,args.samples,args.train_new_model)\n",
        "            end = time.time()\n",
        "            score = 100*roc_auc_score(test_y, test_out)\n",
        "            print('Dataset: %s \\t Samples: %s \\t k: %d \\t Score: %.4f \\t Runtime: %.2f seconds' %(args.dataset,args.samples,args.k,score,(end-start)))\n",
        "    elif mode == 2:\n",
        "        percentages = [50, 70, 90]\n",
        "        for percentage in percentages:\n",
        "            print(f\"Percentage = {percentage} : \")\n",
        "            seed = 3\n",
        "            #print(\"Running trial with random seed = %d\" %seed)\n",
        "            train_x, train_y, val_x, val_y, test_x, test_y = target2_load_dataset(args.dataset,seed,percentage)\n",
        "            start = time.time()\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,args.dataset,seed,args.k,args.samples,args.train_new_model)\n",
        "            end = time.time()\n",
        "            score = 100*roc_auc_score(test_y, test_out)\n",
        "            print('Dataset: %s \\t Samples: %s \\t k: %d \\t Score: %.4f \\t Runtime: %.2f seconds' %(args.dataset,args.samples,args.k,score,(end-start)))\n",
        "\n"
      ],
      "metadata": {
        "id": "iilwC0okTthg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment"
      ],
      "metadata": {
        "id": "0tDK2c7oVBlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target 1\n",
        "Decreasing performance by training on noisy data: add a proportion P% of anomalies into the training set (change P%=10%,30%,50%)"
      ],
      "metadata": {
        "id": "2l9GxNi80-3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def target1_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
        "    val_mask = np.zeros(len(all_train_x))\n",
        "    val_mask[val_idx] = 1\n",
        "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
        "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x[train_y == 0])\n",
        "    train_x = scaler.transform(train_x)\n",
        "    val_x = scaler.transform(val_x)\n",
        "\n",
        "    if all_test_x is None:\n",
        "        test_x = val_x\n",
        "        test_y = val_y\n",
        "\n",
        "    test_x = scaler.transform(all_test_x)\n",
        "    test_y = all_test_y\n",
        "\n",
        "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
        "\n",
        "\n",
        "#load data\n",
        "def target1_load_dataset(dataset,seed, P):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if dataset == 'MI-V':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['06','08','09','10']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
        "        num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
        "\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = np.zeros(len(train_x))\n",
        "        #test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        #test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset == 'MI-F':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['04', '05', '07', '16']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
        "        num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
        "\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = np.zeros(len(train_x))\n",
        "        #test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        #test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
        "        if dataset == 'SHUTTLE':\n",
        "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
        "        elif dataset == 'OPTDIGITS':\n",
        "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
        "        elif dataset == 'PENDIGITS':\n",
        "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = normal_label[train_idx]\n",
        "        #test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        #test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset in ['THYROID','HRSS']:\n",
        "        if dataset == 'THYROID':\n",
        "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
        "        if dataset == 'HRSS':\n",
        "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
        "        label = data[:,-1].astype('float32').squeeze()\n",
        "        data = data[:,:-1].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = normal_label[train_idx]\n",
        "        #test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        #test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset == 'SATELLITE':\n",
        "        data = loadmat('data/SATELLITE/satellite.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data = data[label == 0]\n",
        "        normal_label = label[label ==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "\n",
        "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = normal_data[test_idx]\n",
        "        test_y  = normal_label[test_idx]\n",
        "\n",
        "        selected_anomaly_idx = np.random.choice(remaining_anomaly_idx, len(test_x), replace=False)\n",
        "        test_x = np.concatenate((test_x, anom_data[selected_anomaly_idx]))\n",
        "        test_y = np.concatenate((test_y, anom_label[selected_anomaly_idx]))\n",
        "\n",
        "        #train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "        #test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = normal_label[train_idx]\n",
        "        #test_x = normal_data[test_idx]\n",
        "        #test_y = normal_label[test_idx]\n",
        "        #test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "        #test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
        "        #test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = target1_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y"
      ],
      "metadata": {
        "id": "5lR3XiKl1BZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target 2\n",
        "decreasing performance by reduce the number of training data: reduce the number of normal samples in the training set (change to 50%, 70%, 90% of the original normal samples)\n"
      ],
      "metadata": {
        "id": "ObEhMJiA2V2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_normal_samples(normal_data, percentage):\n",
        "    # Calculate the number of normal samples to keep based on the percentage\n",
        "    num_samples_to_keep = int(len(normal_data) * percentage / 100)\n",
        "    # Randomly select\n",
        "    idx_to_keep = np.random.choice(np.arange(0, len(normal_data)), num_samples_to_keep, replace=False)\n",
        "    return normal_data[idx_to_keep]"
      ],
      "metadata": {
        "id": "iS6j9TgQ3b_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target2_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    val_idx = np.random.choice(np.arange(len(all_train_x)), size=int(0.15 * len(all_train_x)), replace=False)\n",
        "    val_mask = np.zeros(len(all_train_x), dtype=bool)\n",
        "    val_mask[val_idx] = True\n",
        "\n",
        "    # Use np.where to split data based on the boolean mask\n",
        "    val_x, val_y = all_train_x[np.where(val_mask)], all_train_y[np.where(val_mask)]\n",
        "    train_x, train_y = all_train_x[np.where(~val_mask)], all_train_y[np.where(~val_mask)]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x[train_y == 0])\n",
        "    train_x = scaler.transform(train_x)\n",
        "    val_x = scaler.transform(val_x)\n",
        "\n",
        "    if all_test_x is None:\n",
        "        test_x = val_x\n",
        "        test_y = val_y\n",
        "    else:\n",
        "        test_x = scaler.transform(all_test_x)\n",
        "        test_y = all_test_y\n",
        "\n",
        "    return (\n",
        "        train_x.astype('float32'), train_y.astype('float32'),\n",
        "        val_x.astype('float32'), val_y.astype('float32'),\n",
        "        test_x.astype('float32'), test_y.astype('float32')\n",
        "    )"
      ],
      "metadata": {
        "id": "K0LAZYniQRTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target2_load_dataset(dataset,seed,percentage):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if dataset == 'MI-V':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['06','08','09','10']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = np.zeros(len(train_x))\n",
        "\n",
        "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset == 'MI-F':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['04', '05', '07', '16']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = np.zeros(len(train_x))\n",
        "\n",
        "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
        "        if dataset == 'SHUTTLE':\n",
        "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
        "        elif dataset == 'OPTDIGITS':\n",
        "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
        "        elif dataset == 'PENDIGITS':\n",
        "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "\n",
        "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset in ['THYROID','HRSS']:\n",
        "        if dataset == 'THYROID':\n",
        "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
        "        if dataset == 'HRSS':\n",
        "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
        "        label = data[:,-1].astype('float32').squeeze()\n",
        "        data = data[:,:-1].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "\n",
        "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset == 'SATELLITE':\n",
        "        data = loadmat('data/SATELLITE/satellite.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data = data[label == 0]\n",
        "        normal_label = label[label ==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "\n",
        "        test_x = normal_data[test_idx]\n",
        "        test_y = normal_label[test_idx]\n",
        "        test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "        test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
        "        test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y"
      ],
      "metadata": {
        "id": "_SQR3Isy2iER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target 3\n",
        "random subsetting the features => K feature subsets cover all features (e.g., K=3,6,9), then construct K LUNAR models => average the scores of K models to be final outcome\n",
        "\n",
        "HRSS : 20 | MI-F : 58 | MI-V : 58 | OPTDIGITS : 64  \n",
        "PENDIGITS : 16 | SATELLITE : 36 | SHUTTLE : 9 | THYROIDm : 21  "
      ],
      "metadata": {
        "id": "8ZRbahrz2cXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mask2(total_features, subset_size, overlap_percentage, selected):\n",
        "    #First: randomly selected 1 / K (subset_size) features (from selected == 0) -> selected_indices\n",
        "    mask = np.zeros(total_features, dtype=bool)\n",
        "    selected_indices = np.random.choice(np.where(selected == 0)[0], subset_size, replace=False)\n",
        "    #Second: Calculate num_overlap , choose another num_overlap features by np.setdiff1d using selected_indices -> overlap_indices\n",
        "    num_overlap = int((total_features - subset_size) * overlap_percentage)\n",
        "    overlap_indices = np.random.choice(np.setdiff1d(np.arange(total_features), selected_indices), num_overlap, replace=False)\n",
        "    #Third: update selected (where selected_indices) to 1\n",
        "    selected[selected_indices] = 1\n",
        "    #Fourth: add overlap_indices & selected_indices to mask\n",
        "    mask[overlap_indices] = True\n",
        "    mask[selected_indices] = True\n",
        "    return mask"
      ],
      "metadata": {
        "id": "NNFToJ6tbZGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mask1(total_features, subset_size, overlap_percentage):\n",
        "    mask = np.zeros(total_features, dtype=bool)\n",
        "    num_selected = int(subset_size + (total_features - subset_size) * overlap_percentage)\n",
        "    selected_indices = np.random.choice(total_features, num_selected, replace=False)\n",
        "    mask[selected_indices] = True\n",
        "    return mask"
      ],
      "metadata": {
        "id": "06mTSzTAKZAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split training data into train set and validation set\n",
        "def target3_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
        "    val_mask = np.zeros(len(all_train_x))\n",
        "    val_mask[val_idx] = 1\n",
        "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
        "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x[train_y == 0])\n",
        "    train_x = scaler.transform(train_x)\n",
        "    val_x = scaler.transform(val_x)\n",
        "\n",
        "    if all_test_x is None:\n",
        "        test_x = val_x\n",
        "        test_y = val_y\n",
        "\n",
        "    test_x = scaler.transform(all_test_x)\n",
        "    test_y = all_test_y\n",
        "\n",
        "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
        "\n",
        "#load data\n",
        "def target3_load_dataset(dataset,seed,k,samples,train_new_model, K, overlap, mode, P):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if dataset == 'MI-V':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['06','08','09','10']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data = subset_data[normal_idx == 1]\n",
        "            anomaly_data = subset_data[normal_idx == 0]\n",
        "            if mode == 0:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = np.zeros(len(train_x))\n",
        "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
        "                num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
        "\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "            elif mode == 2:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = np.zeros(len(train_x))\n",
        "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    elif dataset == 'MI-F':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['04', '05', '07', '16']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data = subset_data[normal_idx == 1]\n",
        "            anomaly_data = subset_data[normal_idx == 0]\n",
        "            if mode == 0:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = np.zeros(len(train_x))\n",
        "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
        "                num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "            elif mode == 2:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = np.zeros(len(train_x))\n",
        "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
        "        if dataset == 'SHUTTLE':\n",
        "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
        "        elif dataset == 'OPTDIGITS':\n",
        "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
        "        elif dataset == 'PENDIGITS':\n",
        "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data= subset_data[label == 0]\n",
        "            normal_label = label[label==0]\n",
        "            anom_data = subset_data[label == 1]\n",
        "            anom_label = label[label ==1]\n",
        "            if mode == 0:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "            elif mode == 2:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    elif dataset in ['THYROID','HRSS']:\n",
        "        if dataset == 'THYROID':\n",
        "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
        "        if dataset == 'HRSS':\n",
        "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
        "        label = data[:,-1].astype('float32').squeeze()\n",
        "        data = data[:,:-1].astype('float32')\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data= subset_data[label == 0]\n",
        "            normal_label = label[label==0]\n",
        "            anom_data = subset_data[label == 1]\n",
        "            anom_label = label[label ==1]\n",
        "            if mode == 0:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "            elif mode == 2:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    elif dataset == 'SATELLITE':\n",
        "        data = loadmat('data/SATELLITE/satellite.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data= subset_data[label == 0]\n",
        "            normal_label = label[label==0]\n",
        "            anom_data = subset_data[label == 1]\n",
        "            anom_label = label[label ==1]\n",
        "            if mode == 0:\n",
        "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = normal_data[test_idx]\n",
        "                test_y = normal_label[test_idx]\n",
        "                test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "                test_x = np.concatenate((test_x, anom_data[test_idx]))\n",
        "                test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "\n",
        "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = normal_data[test_idx]\n",
        "                test_y  = normal_label[test_idx]\n",
        "\n",
        "                selected_anomaly_idx = np.random.choice(remaining_anomaly_idx, len(test_x), replace=False)\n",
        "                test_x = np.concatenate((test_x, anom_data[selected_anomaly_idx]))\n",
        "                test_y = np.concatenate((test_y, anom_label[selected_anomaly_idx]))\n",
        "            elif mode == 2:\n",
        "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = normal_data[test_idx]\n",
        "                test_y = normal_label[test_idx]\n",
        "                test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "                test_x = np.concatenate((test_x, anom_data[test_idx]))\n",
        "                test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    final_out = torch.mean(torch.stack(test_out_list), dim=0).numpy()\n",
        "    score = 100*roc_auc_score(test_y, final_out)\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "0T3yHiEVTV8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "Qa7iygN3VK3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = [\"HRSS\", \"MI-F\", \"MI-V\", \"OPTDIGITS\", \"PENDIGITS\", \"SATELLITE\", \"SHUTTLE\", \"THYROID\"]\n",
        "\n",
        "#modes = [0, 1, 2]\n",
        "mode = 2\n",
        "P = 90\n",
        "\n",
        "K = 6\n",
        "#overlaps = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "overlap_percentage_values = [0.6, 0.3, 0.3, 0.5, 0.5, 0.5, 0.4, 0.2]\n",
        "overlap_percentages = {dataset: overlap_percentage for dataset, overlap_percentage in zip(datasets, overlap_percentage_values)}\n",
        "\n",
        "k = 100\n",
        "train_new_model = True\n",
        "samples = 'MIXED'\n",
        "\n",
        "#for overlap in overlaps:\n",
        "#  print(f\"Overlap : {overlap} ------------------------------------------------------------------------------------------\")\n",
        "for dataset in datasets:\n",
        "    print(f\"Dataset: {dataset}\")\n",
        "    #train_x, train_y, val_x, val_y, test_x, test_y = load_dataset(dataset,0)\n",
        "    #test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,0,k,samples,True)\n",
        "    #origin_score = 100 * roc_auc_score(test_y, test_out)\n",
        "    scores = []\n",
        "    overlap = overlap_percentages[dataset]\n",
        "    for seed in [1, 2, 3, 4, 5]:\n",
        "        score = target3_load_dataset(dataset,seed,k,samples,train_new_model, K, overlap, mode, P)\n",
        "        print('%.2f ' %(score), end=\"\")\n",
        "        scores.append(score)\n",
        "    average_score = np.mean(scores)\n",
        "    #print('LUNAR : Score: %.2f \\t' %(origin_score))\n",
        "    #print('Subset : Score: %.2f \\t' %(average_score))\n",
        "    print(' Score: %.2f \\t' %(average_score))"
      ],
      "metadata": {
        "id": "09I9eU1iCSY3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c49fe4d-02f5-483d-a3df-c71419ef3ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: HRSS\n",
            "86.65 91.83 89.39 91.09 90.72  Score: 89.93 \t\n",
            "Dataset: MI-F\n",
            "84.17 76.85 85.37 81.82 81.14  Score: 81.87 \t\n",
            "Dataset: MI-V\n",
            "99.21 99.63 99.47 99.08 99.28  Score: 99.33 \t\n",
            "Dataset: OPTDIGITS\n",
            "99.99 100.00 100.00 100.00 100.00  Score: 100.00 \t\n",
            "Dataset: PENDIGITS\n",
            "99.98 100.00 99.99 100.00 100.00  Score: 100.00 \t\n",
            "Dataset: SATELLITE\n",
            "95.58 98.51 97.88 90.40 93.28  Score: 95.13 \t\n",
            "Dataset: SHUTTLE\n",
            "99.98 99.99 99.97 99.96 100.00  Score: 99.98 \t\n",
            "Dataset: THYROID\n",
            "95.31 61.85 92.75 90.22 87.42  Score: 85.51 \t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    k = 100\n",
        "    train_new_model = True\n",
        "    samples = 'MIXED'\n",
        "\n",
        "# List of datasets\n",
        "datasets = [\"HRSS\", \"MI-F\", \"MI-V\", \"OPTDIGITS\", \"PENDIGITS\", \"SATELLITE\", \"SHUTTLE\", \"THYROID\"]\n",
        "modes = [0, 1, 2]"
      ],
      "metadata": {
        "id": "wRnHUviiVntc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the main function\n",
        "# Loop over datasets\n",
        "for mode in modes:\n",
        "  print(f\"Mode {mode}:\")\n",
        "  for dataset in datasets:\n",
        "      print(f\"Dataset: {dataset} ------------------------------------------------------------------------------------------\")\n",
        "      args = Args()\n",
        "      args.dataset = dataset\n",
        "      args.samples = 'MIXED'\n",
        "      args.mode = mode\n",
        "      main(args)"
      ],
      "metadata": {
        "id": "yyQXhQWXVbON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cc0a0e0-643d-40cd-b205-e4f17f8c46ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mode 0:\n",
            "Dataset: HRSS ------------------------------------------------------------------------------------------\n",
            "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 92.5229 \t Runtime: 65.57 seconds\n",
            "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 92.1545 \t Runtime: 66.00 seconds\n",
            "Dataset: MI-F ------------------------------------------------------------------------------------------\n",
            "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 83.8371 \t Runtime: 20.26 seconds\n",
            "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 85.0144 \t Runtime: 19.32 seconds\n",
            "Dataset: MI-V ------------------------------------------------------------------------------------------\n",
            "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 96.7758 \t Runtime: 16.19 seconds\n",
            "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 96.7682 \t Runtime: 15.36 seconds\n",
            "Dataset: OPTDIGITS ------------------------------------------------------------------------------------------\n",
            "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.5067 \t Runtime: 4.03 seconds\n",
            "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.9733 \t Runtime: 3.93 seconds\n",
            "Dataset: PENDIGITS ------------------------------------------------------------------------------------------\n",
            "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.9959 \t Runtime: 3.58 seconds\n",
            "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.3220 \t Runtime: 3.17 seconds\n",
            "Dataset: SATELLITE ------------------------------------------------------------------------------------------\n",
            "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 87.0101 \t Runtime: 2.87 seconds\n",
            "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 83.3098 \t Runtime: 2.82 seconds\n",
            "Dataset: SHUTTLE ------------------------------------------------------------------------------------------\n",
            "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 99.9677 \t Runtime: 27.10 seconds\n",
            "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 99.9835 \t Runtime: 27.29 seconds\n",
            "Dataset: THYROID ------------------------------------------------------------------------------------------\n",
            "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 83.4089 \t Runtime: 3.47 seconds\n",
            "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 87.6457 \t Runtime: 3.57 seconds\n",
            "Mode 1:\n",
            "Dataset: HRSS ------------------------------------------------------------------------------------------\n",
            "Percentage = 10 : \n",
            "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 86.2740 \t Runtime: 69.02 seconds\n",
            "Percentage = 30 : \n",
            "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 77.0009 \t Runtime: 71.49 seconds\n",
            "Percentage = 50 : \n",
            "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 70.4648 \t Runtime: 75.88 seconds\n",
            "Dataset: MI-F ------------------------------------------------------------------------------------------\n",
            "Percentage = 10 : \n",
            "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 49.2165 \t Runtime: 20.35 seconds\n",
            "Percentage = 30 : \n",
            "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 44.8001 \t Runtime: 20.21 seconds\n",
            "Percentage = 50 : \n",
            "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 45.9484 \t Runtime: 21.08 seconds\n",
            "Dataset: MI-V ------------------------------------------------------------------------------------------\n",
            "Percentage = 10 : \n",
            "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 79.7181 \t Runtime: 15.95 seconds\n",
            "Percentage = 30 : \n",
            "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 65.3121 \t Runtime: 17.09 seconds\n",
            "Percentage = 50 : \n",
            "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 60.2135 \t Runtime: 17.30 seconds\n",
            "Dataset: OPTDIGITS ------------------------------------------------------------------------------------------\n",
            "Percentage = 10 : \n",
            "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 97.7558 \t Runtime: 4.02 seconds\n",
            "Percentage = 30 : \n",
            "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 43.0113 \t Runtime: 4.23 seconds\n",
            "Percentage = 50 : \n",
            "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 30.1867 \t Runtime: 3.99 seconds\n",
            "Dataset: PENDIGITS ------------------------------------------------------------------------------------------\n",
            "Percentage = 10 : \n",
            "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 98.4005 \t Runtime: 3.28 seconds\n",
            "Percentage = 30 : \n",
            "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 75.0496 \t Runtime: 3.55 seconds\n",
            "Percentage = 50 : \n",
            "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 29.7173 \t Runtime: 3.24 seconds\n",
            "Dataset: SATELLITE ------------------------------------------------------------------------------------------\n",
            "Percentage = 10 : \n",
            "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 53.8860 \t Runtime: 2.88 seconds\n",
            "Percentage = 30 : \n",
            "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 45.2975 \t Runtime: 3.06 seconds\n",
            "Percentage = 50 : \n",
            "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 49.2817 \t Runtime: 3.59 seconds\n",
            "Dataset: SHUTTLE ------------------------------------------------------------------------------------------\n",
            "Percentage = 10 : \n",
            "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 27.5349 \t Runtime: 27.36 seconds\n",
            "Percentage = 30 : \n",
            "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 90.8203 \t Runtime: 27.99 seconds\n",
            "Percentage = 50 : \n",
            "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 85.7764 \t Runtime: 28.67 seconds\n",
            "Dataset: THYROID ------------------------------------------------------------------------------------------\n",
            "Percentage = 10 : \n",
            "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 83.3027 \t Runtime: 3.69 seconds\n",
            "Percentage = 30 : \n",
            "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 74.0270 \t Runtime: 3.92 seconds\n",
            "Percentage = 50 : \n",
            "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 71.3462 \t Runtime: 3.56 seconds\n",
            "Mode 2:\n",
            "Dataset: HRSS ------------------------------------------------------------------------------------------\n",
            "Percentage = 50 : \n",
            "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 82.4895 \t Runtime: 29.45 seconds\n",
            "Percentage = 70 : \n",
            "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 86.6586 \t Runtime: 44.01 seconds\n",
            "Percentage = 90 : \n",
            "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 90.3914 \t Runtime: 61.64 seconds\n",
            "Dataset: MI-F ------------------------------------------------------------------------------------------\n",
            "Percentage = 50 : \n",
            "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 84.1106 \t Runtime: 9.74 seconds\n",
            "Percentage = 70 : \n",
            "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 82.2215 \t Runtime: 13.79 seconds\n",
            "Percentage = 90 : \n",
            "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 84.1632 \t Runtime: 17.08 seconds\n",
            "Dataset: MI-V ------------------------------------------------------------------------------------------\n",
            "Percentage = 50 : \n",
            "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 95.3318 \t Runtime: 8.57 seconds\n",
            "Percentage = 70 : \n",
            "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 95.6101 \t Runtime: 11.12 seconds\n",
            "Percentage = 90 : \n",
            "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 96.4087 \t Runtime: 14.06 seconds\n",
            "Dataset: OPTDIGITS ------------------------------------------------------------------------------------------\n",
            "Percentage = 50 : \n",
            "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.3911 \t Runtime: 2.21 seconds\n",
            "Percentage = 70 : \n",
            "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.4356 \t Runtime: 2.83 seconds\n",
            "Percentage = 90 : \n",
            "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.4400 \t Runtime: 3.58 seconds\n",
            "Dataset: PENDIGITS ------------------------------------------------------------------------------------------\n",
            "Percentage = 50 : \n",
            "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.8439 \t Runtime: 1.98 seconds\n",
            "Percentage = 70 : \n",
            "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.8397 \t Runtime: 2.58 seconds\n",
            "Percentage = 90 : \n",
            "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.8315 \t Runtime: 2.89 seconds\n",
            "Dataset: SATELLITE ------------------------------------------------------------------------------------------\n",
            "Percentage = 50 : \n",
            "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 84.1433 \t Runtime: 1.80 seconds\n",
            "Percentage = 70 : \n",
            "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 84.6063 \t Runtime: 2.11 seconds\n",
            "Percentage = 90 : \n",
            "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 86.0120 \t Runtime: 2.57 seconds\n",
            "Dataset: SHUTTLE ------------------------------------------------------------------------------------------\n",
            "Percentage = 50 : \n",
            "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 99.8552 \t Runtime: 12.00 seconds\n",
            "Percentage = 70 : \n",
            "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 99.9651 \t Runtime: 17.60 seconds\n",
            "Percentage = 90 : \n",
            "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 99.9390 \t Runtime: 24.12 seconds\n",
            "Dataset: THYROID ------------------------------------------------------------------------------------------\n",
            "Percentage = 50 : \n",
            "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 87.9301 \t Runtime: 2.15 seconds\n",
            "Percentage = 70 : \n",
            "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 85.2263 \t Runtime: 2.74 seconds\n",
            "Percentage = 90 : \n",
            "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 83.6816 \t Runtime: 3.29 seconds\n"
          ]
        }
      ]
    }
  ]
}