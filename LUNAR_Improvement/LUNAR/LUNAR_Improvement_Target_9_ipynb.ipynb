{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2hxIZtRarHN"
      },
      "source": [
        "#**LUNAR_Improvement**\n",
        "Files: https://drive.google.com/drive/folders/1SD1qmuH-5BdNVOEtOXN-gq3aXlTHYz7E?usp=drive_link\n",
        "\n",
        "Paper: https://arxiv.org/pdf/2112.05355.pdf\n",
        "\n",
        "Code & Data: https://github.com/agoodge/LUNAR\n",
        "\n",
        "## Target\n",
        "\n",
        "Tasks1:\n",
        "\n",
        "1. decreasing performance by training on noisy data: add a proportion P% of anomalies into the training set (change P%=10%,30%,50%)\n",
        "\n",
        "2. decreasing performance by reduce the number of training data: reduce the number of normal samples in the training set (change to 50%, 70%, 90% of the original normal samples)\n",
        "\n",
        "3. random subsetting the features => K feature subsets cover all features (e.g., K=3,6,9), then construct K LUNAR models => average the scores of K models to be final outcome\n",
        "\n",
        "Tasks2:\n",
        "\n",
        "**題目 : 如何設計Subset? 兩個方向：( 1 ) 選擇 subset    ( 2 ) subset的KNN的k**\n",
        "4. 減少train的資料數(可以建Graph但不能用來train), 用Target3-1嘗試\n",
        "\n",
        "    Conclusion: 大部分資料只要有訓練資料就無太大變化\n",
        "\n",
        "5. 確認模型的layer層數, 試將模型加成2個layer的GNN\n",
        "  原：歐式距離來算\n",
        "  第二層：歐式距離 (1)做neighbor aggregation or (2)用embedding重新建一個Graph(一樣做歐式距離)\n",
        "\n",
        "6. 用Feature的相關性來分subsets\n",
        "  Ex. HRSS的20個feature做分群(分成3群)\n",
        "\n",
        "  每一群當成一個subsets, (允許overlap?)\n",
        "  20 * 90000\n",
        "  -> 90000 * 20\n",
        "\n",
        "  用90000維的feature做k-means,\n",
        "  設定為3 6 9 群，\n",
        "  變成3 6 9個subsets(利用feature的相似度)\n",
        "\n",
        "7. Think : 有沒有合理的方式評估不同subset的k要設定多少?\n",
        "\n",
        "  一、因為k會影響Graph密度 -> 從小的dataset觀察, 把graph畫出來看多少個neighbor適合\n",
        "\n",
        "  二、算出距離, 距離norm成0 - 1, 用0.1內的neighbor(每個node用的k都不一樣)\n",
        "\n",
        "  三、直接爆train看看\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-EdmcHTH_MY"
      },
      "source": [
        "## Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p5oMIyvrwCH"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1_CeW_EqvqHxYyG7OxwPP9fmMLAWwnyzj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slbLTWGdYNan"
      },
      "source": [
        "## 前置作業"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qnboMxJQr2Tl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd61c260-82a7-45d2-9a56-474bf5b6db7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kUu0ba-HQ0k9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "693a9cce-7661-46ed-c0ae-dff7f1bfebfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "project_folder = '/content/drive/MyDrive/LUNAR'\n",
        "os.chdir(project_folder)\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.1.0%2Bcu121.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.1.0%2Bcu121.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8Gugx2THx7B"
      },
      "source": [
        "#Task 1 Code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "624ffRjLX2ZY"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fzpCS99wUeBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba1253c-a869-43c6-f664-60515c12b107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:85: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:123: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import faiss\n",
        "import argparse\n",
        "from copy import deepcopy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.nn import GATConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import OPTICS\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCEt9ee6YZQd"
      },
      "source": [
        "## 程式碼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX6awK7DYUYO"
      },
      "source": [
        "### variables.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_pz6cKhYUZUt"
      },
      "outputs": [],
      "source": [
        "#device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# model parameters for gnn\n",
        "n_epochs = 200\n",
        "lr = 0.001\n",
        "wd = 0.1\n",
        "\n",
        "# negative sample hyperparameters\n",
        "epsilon = 0.1\n",
        "proportion = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87QEmASwYmUT"
      },
      "source": [
        "### utils.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4jHSTS_ZGe6"
      },
      "source": [
        "NEGATIVE SAMPLE FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xjtkbhBtYwI8"
      },
      "outputs": [],
      "source": [
        "def negative_samples(train_x, train_y, val_x, val_y, test_x, test_y, k, sample_type, proportion, epsilon):\n",
        "\n",
        "    # training set negative samples\n",
        "    neg_train_x, neg_train_y = generate_negative_samples(train_x, sample_type, proportion, epsilon)\n",
        "    # validation set negative samples\n",
        "    neg_val_x, neg_val_y = generate_negative_samples(val_x, sample_type, proportion, epsilon)\n",
        "\n",
        "    # concat data\n",
        "    x = np.vstack((train_x,neg_train_x,val_x,neg_val_x,test_x))\n",
        "    y = np.hstack((train_y,neg_train_y,val_y,neg_val_y,test_y))\n",
        "\n",
        "    # all training set\n",
        "    train_mask = np.hstack((np.ones(len(train_x)),np.ones(len(neg_train_x)),\n",
        "                            np.zeros(len(val_x)),np.zeros(len(neg_val_x)),\n",
        "                            np.zeros(len(test_x))))\n",
        "    # all validation set\n",
        "    val_mask = np.hstack((np.zeros(len(train_x)),np.zeros(len(neg_train_x)),\n",
        "                          np.ones(len(val_x)),np.ones(len(neg_val_x)),\n",
        "                          np.zeros(len(test_x))))\n",
        "    # all test set\n",
        "    test_mask = np.hstack((np.zeros(len(train_x)),np.zeros(len(neg_train_x)),\n",
        "                           np.zeros(len(val_x)),np.zeros(len(neg_val_x)),\n",
        "                           np.ones(len(test_x))))\n",
        "    # normal training points\n",
        "    neighbor_mask = np.hstack((np.ones(len(train_x)), np.zeros(len(neg_train_x)),\n",
        "                               np.zeros(len(val_y)), np.zeros(len(neg_val_x)),\n",
        "                               np.zeros(len(test_y))))\n",
        "\n",
        "    # find k nearest neighbours (idx) and their distances (dist) to each points in x within neighbour_mask==1\n",
        "    dist, idx = find_neighbors(x, y, neighbor_mask, k)\n",
        "\n",
        "    return x.astype('float32'), y.astype('float32'), neighbor_mask.astype('float32'), train_mask.astype('float32'), val_mask.astype('float32'), test_mask.astype('float32'), dist, idx\n",
        "\n",
        "# loading negative samples\n",
        "def generate_negative_samples(x, sample_type, proportion, epsilon):\n",
        "\n",
        "    n_samples = int(proportion*(len(x)))\n",
        "    n_dim = x.shape[-1]\n",
        "\n",
        "    #M\n",
        "    randmat = np.random.rand(n_samples,n_dim) < 0.3\n",
        "    # uniform samples\n",
        "    rand_unif = (epsilon* (1-2*np.random.rand(n_samples,n_dim)))\n",
        "    #  subspace perturbation samples\n",
        "    rand_sub = np.tile(x, (proportion,1)) + randmat*(epsilon*np.random.randn(n_samples,n_dim))\n",
        "\n",
        "    if sample_type == 'UNIFORM':\n",
        "        neg_x = rand_unif\n",
        "    if sample_type == 'SUBSPACE':\n",
        "        neg_x = rand_sub\n",
        "    if sample_type == 'MIXED':\n",
        "        # randomly sample from uniform and gaussian negative samples\n",
        "        neg_x = np.concatenate((rand_unif, rand_sub),0)\n",
        "        neg_x = neg_x[np.random.choice(np.arange(len(neg_x)), size = n_samples)]\n",
        "\n",
        "    neg_y = np.ones(len(neg_x))\n",
        "\n",
        "    return neg_x.astype('float32'), neg_y.astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iayk7CJxZTmP"
      },
      "source": [
        "GRAPH FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bqwfA3ZRY3Eg"
      },
      "outputs": [],
      "source": [
        "# find the k nearest neighbours of all x points out of the neighbour candidates\n",
        "def find_neighbors(x, y, neighbor_mask, k):\n",
        "\n",
        "    # nearest neighbour object\n",
        "    index = faiss.IndexFlatL2(x.shape[-1])\n",
        "    # add nearest neighbour candidates\n",
        "    index.add(x[neighbor_mask==1])\n",
        "\n",
        "    # distances and idx of neighbour points for the neighbour candidates (k+1 as the first one will be the point itself)\n",
        "    dist_train, idx_train = index.search(x[neighbor_mask==1], k = k+1)\n",
        "    # remove 1st nearest neighbours to remove self loops\n",
        "    dist_train, idx_train = dist_train[:,1:], idx_train[:,1:]\n",
        "    # distances and idx of neighbour points for the non-neighbour candidates\n",
        "    dist_test, idx_test = index.search(x[neighbor_mask==0], k = k)\n",
        "    #concat\n",
        "    dist = np.vstack((dist_train, dist_test))\n",
        "    idx = np.vstack((idx_train, idx_test))\n",
        "\n",
        "    return dist, idx\n",
        "\n",
        "# create graph object out of x, y, distances and indices of neighbours\n",
        "def build_graph(x, y, dist, idx):\n",
        "\n",
        "    # array like [0,0,0,0,0,1,1,1,1,1,...,n,n,n,n,n] for k = 5 (i.e. edges sources)\n",
        "    idx_source = np.repeat(np.arange(len(x)),dist.shape[-1]).astype('int32')\n",
        "    idx_source = np.expand_dims(idx_source,axis=0)\n",
        "\n",
        "    # edge targets, i.e. the nearest k neighbours of point 0, 1,..., n\n",
        "    idx_target = idx.flatten()\n",
        "    idx_target = np.expand_dims(idx_target,axis=0).astype('int32')\n",
        "\n",
        "    #stack source and target indices\n",
        "    idx = np.vstack((idx_source, idx_target))\n",
        "\n",
        "    # edge weights\n",
        "    attr = dist.flatten()\n",
        "    attr = np.sqrt(attr)\n",
        "    attr = np.expand_dims(attr, axis=1)\n",
        "\n",
        "    # into tensors\n",
        "    x = torch.tensor(x, dtype = torch.float32)\n",
        "    y = torch.tensor(y,dtype = torch.float32)\n",
        "    idx = torch.tensor(idx, dtype = torch.long)\n",
        "    attr = torch.tensor(attr, dtype = torch.float32)\n",
        "\n",
        "    #build PyTorch geometric Data object\n",
        "    data = Data(x = x, edge_index = idx, edge_attr = attr, y = y)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm41pHtcZWGn"
      },
      "source": [
        "DATASET FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hCR2GGmmUwoM"
      },
      "outputs": [],
      "source": [
        "# split training data into train set and validation set\n",
        "def split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
        "    val_mask = np.zeros(len(all_train_x))\n",
        "    val_mask[val_idx] = 1\n",
        "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
        "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x[train_y == 0])\n",
        "    train_x = scaler.transform(train_x)\n",
        "    val_x = scaler.transform(val_x)\n",
        "\n",
        "    if all_test_x is None:\n",
        "        test_x = val_x\n",
        "        test_y = val_y\n",
        "\n",
        "    test_x = scaler.transform(all_test_x)\n",
        "    test_y = all_test_y\n",
        "\n",
        "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
        "\n",
        "\n",
        "#load data\n",
        "def load_dataset(dataset,seed):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if dataset == 'MI-V':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['06','08','09','10']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = np.zeros(len(train_x))\n",
        "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset == 'MI-F':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['04', '05', '07', '16']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = np.zeros(len(train_x))\n",
        "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
        "        if dataset == 'SHUTTLE':\n",
        "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
        "        elif dataset == 'OPTDIGITS':\n",
        "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
        "        elif dataset == 'PENDIGITS':\n",
        "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset in ['THYROID','HRSS']:\n",
        "        if dataset == 'THYROID':\n",
        "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
        "        if dataset == 'HRSS':\n",
        "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
        "        label = data[:,-1].astype('float32').squeeze()\n",
        "        data = data[:,:-1].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset == 'SATELLITE':\n",
        "        data = loadmat('data/SATELLITE/satellite.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data = data[label == 0]\n",
        "        normal_label = label[label ==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "        train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "        test_x = normal_data[test_idx]\n",
        "        test_y = normal_label[test_idx]\n",
        "        test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "        test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
        "        test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaD6MtKrZYYe"
      },
      "source": [
        "### lunar.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout=0.1):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "    self.linear1 = nn.Linear(embed_dim, feed_forward_dim)\n",
        "    self.linear2 = nn.Linear(feed_forward_dim, embed_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
        "    self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "  def forward(self, src, src_mask, tgt, tgt_mask=None):\n",
        "    src = self.self_attn(src, src, src, attn_mask=src_mask)[0]\n",
        "    src = self.dropout(src) + src\n",
        "    src = self.layer_norm1(src)\n",
        "    src = src.transpose(0, 1)\n",
        "\n",
        "    tgt = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
        "    tgt = self.dropout(tgt) + tgt\n",
        "    tgt = self.layer_norm2(tgt)\n",
        "    return src, tgt"
      ],
      "metadata": {
        "id": "6bioF3AkCfLA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FTTransformerLayer(nn.Module):\n",
        "    def __init__(self, input_size, num_heads):\n",
        "        super(FTTransformerLayer, self).__init__()\n",
        "        self.transformer = nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)  # Adding a sequence dimension\n",
        "        x = self.transformer(x)\n",
        "        x = x.squeeze(1)  # Removing the sequence dimension\n",
        "        return x"
      ],
      "metadata": {
        "id": "bBvIkI6HMm5a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.gnn1 = GNN1(k)\n",
        "        self.attention = GATConv(in_channels=32, out_channels=32, heads=4, concat=False)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        gnn_output = self.gnn1(x, edge_index, edge_attr)\n",
        "        att_output = self.attention(gnn_output, edge_index)\n",
        "        return att_output"
      ],
      "metadata": {
        "id": "YImvCOyAFpqS"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionLayer(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SelfAttentionLayer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is of shape (n, embed_dim)\n",
        "        Q = self.query(x)  # shape: (n, embed_dim)\n",
        "        K = self.key(x)    # shape: (n, embed_dim)\n",
        "        V = self.value(x)  # shape: (n, embed_dim)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.embed_dim, dtype=torch.float32))\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attention_weights = self.softmax(attention_scores)  # shape: (n, n)\n",
        "\n",
        "        # Calculate the output embeddings\n",
        "        out = torch.matmul(attention_weights, V)  # shape: (n, embed_dim)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Krb3vjMqU9Az"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YRNnluAPHmI2"
      },
      "outputs": [],
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.network(x)\n",
        "        return out\n",
        "\n",
        "# Message passing scheme\n",
        "class GNN1(MessagePassing):\n",
        "    def __init__(self,k):\n",
        "        super(GNN1, self).__init__(flow=\"target_to_source\")\n",
        "        self.k = k\n",
        "        self.hidden_size = 256\n",
        "        self.embedding_size = 32\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(k,self.hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden_size,self.hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden_size,self.hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(self.hidden_size,self.embedding_size),\n",
        "            nn.Tanh(),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        self.network = self.network.to(dtype = torch.float32)\n",
        "        out = self.propagate(edge_index = edge_index, x=x, edge_attr=edge_attr, k = self.k, network=self.network)\n",
        "        return out\n",
        "\n",
        "    def message(self,x_i,x_j,edge_attr):\n",
        "        # message is the edge weight\n",
        "        return edge_attr\n",
        "\n",
        "    def aggregate(self, inputs, index, k, network):\n",
        "        # concatenate all k messages\n",
        "        self.input_aggr = inputs.reshape(-1,k)\n",
        "        # pass through network\n",
        "        out = self.network(self.input_aggr)\n",
        "        return out\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, k):\n",
        "        super(GNN, self).__init__()\n",
        "        self.k = k\n",
        "        self.L1 = GNN1(self.k)\n",
        "        self.transformer = FTTransformerLayer(input_size=32, num_heads=4)\n",
        "        #self.self_attention = SelfAttention(self.k)\n",
        "        #self.self_attention = SelfAttentionLayer(embed_dim=32)\n",
        "        self.MLP = MLP(input_size=32, hidden_size=32, output_size=1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        self.edge_attr = data.edge_attr\n",
        "        self.edge_index = data.edge_index\n",
        "        self.x = data.x\n",
        "\n",
        "        # GNN1 output\n",
        "        out1 = self.L1(self.x, self.edge_index, self.edge_attr)\n",
        "\n",
        "        # FTTransformerLayer output\n",
        "        out2 = self.transformer(out1)\n",
        "        #out2 = self.self_attention(out1, self.edge_index, self.edge_attr)\n",
        "        #out2 = self.self_attention(out1)\n",
        "        # MLP output\n",
        "        out3 = self.MLP(out2)\n",
        "        #out3 = self.MLP(out1)\n",
        "        out3 = torch.squeeze(out3, 1)\n",
        "        #out3 = torch.squeeze(out2, 1)\n",
        "        return out3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hQVfnAaBU3ee"
      },
      "outputs": [],
      "source": [
        "def run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model):\n",
        "\n",
        "    # loss function\n",
        "    criterion = nn.MSELoss(reduction = 'none')\n",
        "\n",
        "    # path to save model parameters\n",
        "    model_path = 'saved_models/%s/%d/net_%d.pth' %(dataset,k,seed)\n",
        "    if not os.path.exists(os.path.dirname(model_path)):\n",
        "       os.makedirs(os.path.dirname(model_path))\n",
        "\n",
        "    x, y, neighbor_mask, train_mask, val_mask, test_mask, dist, idx = negative_samples(train_x, train_y, val_x, val_y, test_x, test_y, k, samples, proportion, epsilon)\n",
        "\n",
        "    data = build_graph(x, y, dist, idx)\n",
        "    data = data.to(device)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    concat_feature = x;\n",
        "    feature_size = concat_feature.shape[1]\n",
        "    #net = GNN(k, feature_size).to(device)\n",
        "    net = GNN(k).to(device)\n",
        "\n",
        "    if train_new_model == True:\n",
        "\n",
        "        optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay = wd)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            net.eval()\n",
        "            out = net(data)\n",
        "            #out = net(data, concat_feature)\n",
        "            loss = criterion(out,data.y)\n",
        "\n",
        "            val_loss = loss[val_mask == 1].mean()\n",
        "            val_score = roc_auc_score(data.y[val_mask==1].cpu(),out[val_mask==1].cpu())\n",
        "\n",
        "            best_val_score = 0\n",
        "\n",
        "        # training\n",
        "        for epoch in range(n_epochs):\n",
        "            net.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = net(data)\n",
        "            #out = net(data, concat_feature)\n",
        "            # loss for training data only\n",
        "            loss = criterion(out[train_mask == 1],data.y[train_mask == 1]).sum()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                net.eval()\n",
        "                out = net(data)\n",
        "                #out = net(data, concat_feature)\n",
        "                loss = criterion(out,data.y)\n",
        "                #print(loss.mean())\n",
        "                val_loss = loss[val_mask == 1].mean()\n",
        "                val_score = roc_auc_score(data.y[val_mask==1].cpu(),out[val_mask==1].cpu())\n",
        "\n",
        "                # if new model gives the best validation set score\n",
        "                if val_score >= best_val_score:\n",
        "\n",
        "                    # save model parameters\n",
        "                    best_dict = {'epoch': epoch,\n",
        "                           'model_state_dict': deepcopy(net.state_dict()),\n",
        "                           'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
        "                           'val_loss': val_loss,\n",
        "                           'val_score': val_score,\n",
        "                           'k': k,}\n",
        "\n",
        "                    # save best model\n",
        "                    #torch.save(best_dict, model_path)\n",
        "\n",
        "                    # reset best score so far\n",
        "                    best_val_score = val_score\n",
        "\n",
        "        # load best model\n",
        "        net.load_state_dict(best_dict['model_state_dict'])\n",
        "\n",
        "    # if not training a new model, load the saved model\n",
        "    if train_new_model == False:\n",
        "\n",
        "        load_dict = torch.load(model_path)\n",
        "        net.load_state_dict(load_dict['model_state_dict'])\n",
        "\n",
        "    # testing\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        out = net(data)\n",
        "        #out = net(data, concat_feature)\n",
        "        loss = criterion(out,data.y)\n",
        "        # return output for test points\n",
        "    return out[test_mask==1].cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIePvIj5Zmwk"
      },
      "source": [
        "### main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iilwC0okTthg"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    dataset = args.dataset\n",
        "    samples = args.samples\n",
        "    k = args.k\n",
        "    train_new_model = args.train_new_model\n",
        "    mode = args.mode\n",
        "\n",
        "    Ps = [10, 30, 50]\n",
        "    for P in Ps:\n",
        "        print(f\"Percentage = {P} : \")\n",
        "        seed = 0\n",
        "        #print(\"Running trial with random seed = %d\" %seed)\n",
        "        train_x, train_y, val_x, val_y, test_x, test_y = target1_load_dataset(args.dataset,seed, P)\n",
        "        test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,args.dataset,seed,args.k,args.samples,args.train_new_model)\n",
        "        score = 100*roc_auc_score(test_y, test_out)\n",
        "        print('Score: %.2f' %(score))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tDK2c7oVBlZ"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l9GxNi80-3y"
      },
      "source": [
        "### Target 1\n",
        "Decreasing performance by training on noisy data: add a proportion P% of anomalies into the training set (change P%=10%,30%,50%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5lR3XiKl1BZa"
      },
      "outputs": [],
      "source": [
        "def target1_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
        "    val_mask = np.zeros(len(all_train_x))\n",
        "    val_mask[val_idx] = 1\n",
        "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
        "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x[train_y == 0])\n",
        "    train_x = scaler.transform(train_x)\n",
        "    val_x = scaler.transform(val_x)\n",
        "\n",
        "    if all_test_x is None:\n",
        "        test_x = val_x\n",
        "        test_y = val_y\n",
        "\n",
        "    test_x = scaler.transform(all_test_x)\n",
        "    test_y = all_test_y\n",
        "\n",
        "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
        "\n",
        "\n",
        "#load data\n",
        "def target1_load_dataset(dataset,seed,k,samples,train_new_model, P):\n",
        "    np.random.seed(seed)\n",
        "    if dataset == 'MI-V':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['06','08','09','10']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        data = np.array(data, dtype=float)\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
        "        num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
        "\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = np.zeros(len(train_x))\n",
        "        #test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        #test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset == 'MI-F':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['04', '05', '07', '16']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        data = np.array(data, dtype=float)\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
        "        num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
        "\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = np.zeros(len(train_x))\n",
        "        #test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        #test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
        "        if dataset == 'SHUTTLE':\n",
        "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
        "        elif dataset == 'OPTDIGITS':\n",
        "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
        "        elif dataset == 'PENDIGITS':\n",
        "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = normal_label[train_idx]\n",
        "        #test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        #test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset in ['THYROID','HRSS']:\n",
        "        if dataset == 'THYROID':\n",
        "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
        "        if dataset == 'HRSS':\n",
        "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
        "        label = data[:,-1].astype('float32').squeeze()\n",
        "        data = data[:,:-1].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = normal_label[train_idx]\n",
        "        #test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        #test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset == 'SATELLITE':\n",
        "        data = loadmat('data/SATELLITE/satellite.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data = data[label == 0]\n",
        "        normal_label = label[label ==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "\n",
        "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "\n",
        "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "        test_x = normal_data[test_idx]\n",
        "        test_y  = normal_label[test_idx]\n",
        "\n",
        "        selected_anomaly_idx = np.random.choice(remaining_anomaly_idx, len(test_x), replace=False)\n",
        "        test_x = np.concatenate((test_x, anom_data[selected_anomaly_idx]))\n",
        "        test_y = np.concatenate((test_y, anom_label[selected_anomaly_idx]))\n",
        "\n",
        "        #train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "        #test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        #train_y = normal_label[train_idx]\n",
        "        #test_x = normal_data[test_idx]\n",
        "        #test_y = normal_label[test_idx]\n",
        "        #test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "        #test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
        "        #test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = target1_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "    test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "    score = 100*roc_auc_score(test_y, test_out)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target 2\n",
        "decreasing performance by reduce the number of training data: reduce the number of normal samples in the training set (change to 50%, 70%, 90% of the original normal samples)\n"
      ],
      "metadata": {
        "id": "ObEhMJiA2V2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_normal_samples(normal_data, percentage):\n",
        "    # Calculate the number of normal samples to keep based on the percentage\n",
        "    num_samples_to_keep = int(len(normal_data) * percentage / 100)\n",
        "    # Randomly select\n",
        "    idx_to_keep = np.random.choice(np.arange(0, len(normal_data)), num_samples_to_keep, replace=False)\n",
        "    return normal_data[idx_to_keep]"
      ],
      "metadata": {
        "id": "iS6j9TgQ3b_o"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target2_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    val_idx = np.random.choice(np.arange(len(all_train_x)), size=int(0.15 * len(all_train_x)), replace=False)\n",
        "    val_mask = np.zeros(len(all_train_x), dtype=bool)\n",
        "    val_mask[val_idx] = True\n",
        "\n",
        "    # Use np.where to split data based on the boolean mask\n",
        "    val_x, val_y = all_train_x[np.where(val_mask)], all_train_y[np.where(val_mask)]\n",
        "    train_x, train_y = all_train_x[np.where(~val_mask)], all_train_y[np.where(~val_mask)]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x[train_y == 0])\n",
        "    train_x = scaler.transform(train_x)\n",
        "    val_x = scaler.transform(val_x)\n",
        "\n",
        "    if all_test_x is None:\n",
        "        test_x = val_x\n",
        "        test_y = val_y\n",
        "    else:\n",
        "        test_x = scaler.transform(all_test_x)\n",
        "        test_y = all_test_y\n",
        "\n",
        "    return (\n",
        "        train_x.astype('float32'), train_y.astype('float32'),\n",
        "        val_x.astype('float32'), val_y.astype('float32'),\n",
        "        test_x.astype('float32'), test_y.astype('float32')\n",
        "    )"
      ],
      "metadata": {
        "id": "K0LAZYniQRTQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target2_load_dataset(dataset,seed,percentage):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if dataset == 'MI-V':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['06','08','09','10']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = np.zeros(len(train_x))\n",
        "\n",
        "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset == 'MI-F':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['04', '05', '07', '16']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy()\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "        normal_data = data[normal_idx == 1]\n",
        "        anomaly_data = data[normal_idx == 0]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = np.zeros(len(train_x))\n",
        "\n",
        "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
        "        if dataset == 'SHUTTLE':\n",
        "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
        "        elif dataset == 'OPTDIGITS':\n",
        "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
        "        elif dataset == 'PENDIGITS':\n",
        "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "\n",
        "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset in ['THYROID','HRSS']:\n",
        "        if dataset == 'THYROID':\n",
        "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
        "        if dataset == 'HRSS':\n",
        "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
        "        label = data[:,-1].astype('float32').squeeze()\n",
        "        data = data[:,:-1].astype('float32')\n",
        "        normal_data= data[label == 0]\n",
        "        normal_label = label[label==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "\n",
        "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "    elif dataset == 'SATELLITE':\n",
        "        data = loadmat('data/SATELLITE/satellite.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "        normal_data = data[label == 0]\n",
        "        normal_label = label[label ==0]\n",
        "        anom_data = data[label == 1]\n",
        "        anom_label = label[label ==1]\n",
        "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "\n",
        "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
        "        #train_x = normal_data[train_idx]\n",
        "        train_y = normal_label[train_idx]\n",
        "\n",
        "        test_x = normal_data[test_idx]\n",
        "        test_y = normal_label[test_idx]\n",
        "        test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "        test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
        "        test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "\n",
        "    train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "    return train_x, train_y, val_x, val_y, test_x, test_y"
      ],
      "metadata": {
        "id": "_SQR3Isy2iER"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTBh11pjFXY0"
      },
      "source": [
        "### Target 3\n",
        "random subsetting the features => K feature subsets cover all features (e.g., K=3,6,9), then construct K LUNAR models => average the scores of K models to be final outcome\n",
        "\n",
        "HRSS : 20 | MI-F : 58 | MI-V : 58 | OPTDIGITS : 64  \n",
        "PENDIGITS : 16 | SATELLITE : 36 | SHUTTLE : 9 | THYROIDm : 21  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hOxEi2urFdEo"
      },
      "outputs": [],
      "source": [
        "def generate_mask2(total_features, subset_size, overlap_percentage, selected):\n",
        "    #First: randomly selected 1 / K (subset_size) features (from selected == 0) -> selected_indices\n",
        "    mask = np.zeros(total_features, dtype=bool)\n",
        "    selected_indices = np.random.choice(np.where(selected == 0)[0], subset_size, replace=False)\n",
        "\n",
        "    #Second: update selected (where selected_indices) to 1\n",
        "    selected[selected_indices] = 1\n",
        "    #Third: add selected_indices to mask\n",
        "    mask[selected_indices] = True\n",
        "\n",
        "    #Fourth: Calculate num_overlap , choose another num_overlap features by np.setdiff1d using selected_indices -> overlap_indices\n",
        "    if overlap_percentage != 0:\n",
        "      num_overlap = int((total_features - subset_size) * overlap_percentage)\n",
        "      overlap_indices = np.random.choice(np.setdiff1d(np.arange(total_features), selected_indices), num_overlap, replace=False)\n",
        "      mask[overlap_indices] = True\n",
        "\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qsGwEe4FFeh4"
      },
      "outputs": [],
      "source": [
        "# split training data into train set and validation set\n",
        "def target3_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
        "    val_mask = np.zeros(len(all_train_x))\n",
        "    val_mask[val_idx] = 1\n",
        "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
        "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(train_x[train_y == 0])\n",
        "    train_x = scaler.transform(train_x)\n",
        "    val_x = scaler.transform(val_x)\n",
        "\n",
        "    if all_test_x is None:\n",
        "        test_x = val_x\n",
        "        test_y = val_y\n",
        "\n",
        "    test_x = scaler.transform(all_test_x)\n",
        "    test_y = all_test_y\n",
        "\n",
        "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
        "\n",
        "#load data\n",
        "def target3_load_dataset(dataset,seed,k,samples,train_new_model, K, overlap, mode, P):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    if dataset == 'MI-V':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['06','08','09','10']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy().astype(float)\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data = subset_data[normal_idx == 1]\n",
        "            anomaly_data = subset_data[normal_idx == 0]\n",
        "            if mode == 0:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = np.zeros(len(train_x))\n",
        "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
        "                num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
        "\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "            elif mode == 2:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = np.zeros(len(train_x))\n",
        "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    elif dataset == 'MI-F':\n",
        "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
        "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "        normal_idx = np.ones(len(df))\n",
        "        for i in ['04', '05', '07', '16']:\n",
        "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
        "            #df = df.append(data, ignore_index = True)\n",
        "            df = pd.concat([df,data], ignore_index = True)\n",
        "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
        "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
        "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
        "        data = df.to_numpy().astype(float)\n",
        "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
        "        data = data[idx]\n",
        "        normal_idx = normal_idx[idx]\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data = subset_data[normal_idx == 1]\n",
        "            anomaly_data = subset_data[normal_idx == 0]\n",
        "            if mode == 0:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = np.zeros(len(train_x))\n",
        "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
        "                num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "            elif mode == 2:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = np.zeros(len(train_x))\n",
        "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
        "\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
        "        if dataset == 'SHUTTLE':\n",
        "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
        "        elif dataset == 'OPTDIGITS':\n",
        "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
        "        elif dataset == 'PENDIGITS':\n",
        "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data= subset_data[label == 0]\n",
        "            normal_label = label[label==0]\n",
        "            anom_data = subset_data[label == 1]\n",
        "            anom_label = label[label ==1]\n",
        "            if mode == 0:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "            elif mode == 2:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    elif dataset in ['THYROID','HRSS']:\n",
        "        if dataset == 'THYROID':\n",
        "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
        "        if dataset == 'HRSS':\n",
        "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
        "        label = data[:,-1].astype('float32').squeeze()\n",
        "        data = data[:,:-1].astype('float32')\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data= subset_data[label == 0]\n",
        "            normal_label = label[label==0]\n",
        "            anom_data = subset_data[label == 1]\n",
        "            anom_label = label[label ==1]\n",
        "            if mode == 0:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
        "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
        "            elif mode == 2:\n",
        "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
        "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
        "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    elif dataset == 'SATELLITE':\n",
        "        data = loadmat('data/SATELLITE/satellite.mat')\n",
        "        label = data['y'].astype('float32').squeeze()\n",
        "        data = data['X'].astype('float32')\n",
        "\n",
        "        total_features = data.shape[1]\n",
        "        subset_size = int(total_features / K)\n",
        "        test_out_list = []\n",
        "        selected = np.zeros(total_features, dtype=bool)\n",
        "        for i in range(K):\n",
        "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
        "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
        "            subset_data = data[:, mask]\n",
        "\n",
        "            normal_data= subset_data[label == 0]\n",
        "            normal_label = label[label==0]\n",
        "            anom_data = subset_data[label == 1]\n",
        "            anom_label = label[label ==1]\n",
        "            if mode == 0:\n",
        "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "                train_x = normal_data[train_idx]\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = normal_data[test_idx]\n",
        "                test_y = normal_label[test_idx]\n",
        "                test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "                test_x = np.concatenate((test_x, anom_data[test_idx]))\n",
        "                test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "            elif mode == 1:\n",
        "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
        "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
        "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
        "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
        "\n",
        "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "\n",
        "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
        "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
        "                test_x = normal_data[test_idx]\n",
        "                test_y  = normal_label[test_idx]\n",
        "\n",
        "                selected_anomaly_idx = np.random.choice(remaining_anomaly_idx, len(test_x), replace=False)\n",
        "                test_x = np.concatenate((test_x, anom_data[selected_anomaly_idx]))\n",
        "                test_y = np.concatenate((test_y, anom_label[selected_anomaly_idx]))\n",
        "            elif mode == 2:\n",
        "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
        "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
        "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
        "                train_y = normal_label[train_idx]\n",
        "                test_x = normal_data[test_idx]\n",
        "                test_y = normal_label[test_idx]\n",
        "                test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
        "                test_x = np.concatenate((test_x, anom_data[test_idx]))\n",
        "                test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
        "            if mode == 2:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            else:\n",
        "              train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
        "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
        "\n",
        "            test_out_list.append(test_out)\n",
        "\n",
        "    final_out = torch.mean(torch.stack(test_out_list), dim=0).numpy()\n",
        "    score = 100*roc_auc_score(test_y, final_out)\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa7iygN3VK3u"
      },
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#overlaps = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "\n",
        "mode = 2\n",
        "Ps = [50, 70, 90]\n",
        "Ks = [3,6,9]\n",
        "k = 100\n",
        "train_new_model = True\n",
        "samples = 'MIXED'\n",
        "#datasets = [\"HRSS\", \"MI-F\", \"MI-V\", \"OPTDIGITS\", \"PENDIGITS\", \"SATELLITE\", \"SHUTTLE\", \"THYROID\"]\n",
        "datasets = [\"MI-F\"]\n",
        "\n",
        "seeds = [0]\n",
        "overlap = 0.5\n",
        "\n",
        "for dataset in datasets:\n",
        "    print(f\"Dataset: {dataset}\")\n",
        "    for P in Ps:\n",
        "      print(f\"P: {P}\")\n",
        "      for K in Ks:\n",
        "        print(f\"K: {K}\")\n",
        "        #overlap = overlap_percentages[dataset]\n",
        "        scores = []\n",
        "        for seed in seeds:\n",
        "            score = target3_load_dataset(dataset, seed, k, samples, train_new_model, K, overlap, mode, P)\n",
        "            scores.append(score)\n",
        "        average_score = sum(scores) / len(scores)\n",
        "        print('Average Score: %.2f \\t' % (average_score))\n",
        "        # print(' Score: %.2f \\t' %(score))"
      ],
      "metadata": {
        "id": "NgDjY9dqmO4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c130f25a-5f93-4e24-c022-ef7de27b31b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: MI-F\n",
            "P: 50\n",
            "K: 3\n",
            "Average Score: 84.31 \t\n",
            "K: 6\n",
            "Average Score: 85.94 \t\n",
            "K: 9\n",
            "Average Score: 78.77 \t\n",
            "P: 70\n",
            "K: 3\n",
            "Average Score: 79.83 \t\n",
            "K: 6\n",
            "Average Score: 86.06 \t\n",
            "K: 9\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "G-EdmcHTH_MY",
        "slbLTWGdYNan",
        "FX6awK7DYUYO",
        "87QEmASwYmUT",
        "KIePvIj5Zmwk",
        "2l9GxNi80-3y"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}