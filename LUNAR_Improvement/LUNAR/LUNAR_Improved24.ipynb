{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2hxIZtRarHN"
   },
   "source": [
    "## LUNAR\n",
    "\n",
    "Paper: https://arxiv.org/pdf/2112.05355.pdf\n",
    "\n",
    "Code & Data: https://github.com/agoodge/LUNAR\n",
    "\n",
    "### Target\n",
    "\n",
    "1. decreasing performance by training on noisy data: add a proportion P% of anomalies into the training set (change P%=10%,30%,50%)\n",
    "\n",
    "2. decreasing performance by reduce the number of training data: reduce the number of normal samples in the training set (change to 50%, 70%, 90% of the original normal samples)\n",
    "\n",
    "3. random subsetting the features => K feature subsets cover all features (e.g., K=3,6,9), then construct K LUNAR models => average the scores of K models to be final outcome\n",
    "\n",
    "### 2 papers to read:\n",
    "\n",
    "https://arxiv.org/pdf/2106.11959.pdf\n",
    "\n",
    "https://github.com/yandex-research/rtdl-revisiting-models\n",
    "\n",
    "https://arxiv.org/pdf/2203.05556.pdf\n",
    "\n",
    "https://github.com/yandex-research/rtdl-num-embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slbLTWGdYNan"
   },
   "source": [
    "### 前置作業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXEDoEj38WAA"
   },
   "outputs": [],
   "source": [
    "#!pip install gdown\n",
    "\n",
    "#!gdown --id 1SD1qmuH-5BdNVOEtOXN-gq3aXlTHYz7E -O LUNAR.zip\n",
    "\n",
    "#!unzip LUNAR.zip -d /content/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qnboMxJQr2Tl",
    "outputId": "5133e281-e132-40cc-d5fb-08821c7ae738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUu0ba-HQ0k9",
    "outputId": "b1b2b726-f6ef-4e2f-fecf-c4b63f0583f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 2] 系統找不到指定的檔案。 while executing command git version\n",
      "ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?\n",
      "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
      "ERROR: No matching distribution found for faiss-gpu\n"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#project_folder = '/content/drive/MyDrive/LUNAR'\n",
    "#os.chdir(project_folder)\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-2.2.0%2Bcu118.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-2.2.0%2Bcu118.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "624ffRjLX2ZY"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fzpCS99wUeBb"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'faiss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfaiss\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'faiss'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import faiss\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCEt9ee6YZQd"
   },
   "source": [
    "## 程式碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FX6awK7DYUYO"
   },
   "source": [
    "### variables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pz6cKhYUZUt"
   },
   "outputs": [],
   "source": [
    "#device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model parameters for gnn\n",
    "n_epochs = 200\n",
    "lr = 0.001\n",
    "wd = 0.1\n",
    "\n",
    "# negative sample hyperparameters\n",
    "epsilon = 0.1\n",
    "proportion = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87QEmASwYmUT"
   },
   "source": [
    "### utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4jHSTS_ZGe6"
   },
   "source": [
    "NEGATIVE SAMPLE FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjtkbhBtYwI8"
   },
   "outputs": [],
   "source": [
    "def negative_samples(train_x, train_y, val_x, val_y, test_x, test_y, k, sample_type, proportion, epsilon):\n",
    "\n",
    "    # training set negative samples\n",
    "    neg_train_x, neg_train_y = generate_negative_samples(train_x, sample_type, proportion, epsilon)\n",
    "    # validation set negative samples\n",
    "    neg_val_x, neg_val_y = generate_negative_samples(val_x, sample_type, proportion, epsilon)\n",
    "\n",
    "    # concat data\n",
    "    x = np.vstack((train_x,neg_train_x,val_x,neg_val_x,test_x))\n",
    "    y = np.hstack((train_y,neg_train_y,val_y,neg_val_y,test_y))\n",
    "\n",
    "    # all training set\n",
    "    train_mask = np.hstack((np.ones(len(train_x)),np.ones(len(neg_train_x)),\n",
    "                            np.zeros(len(val_x)),np.zeros(len(neg_val_x)),\n",
    "                            np.zeros(len(test_x))))\n",
    "    # all validation set\n",
    "    val_mask = np.hstack((np.zeros(len(train_x)),np.zeros(len(neg_train_x)),\n",
    "                          np.ones(len(val_x)),np.ones(len(neg_val_x)),\n",
    "                          np.zeros(len(test_x))))\n",
    "    # all test set\n",
    "    test_mask = np.hstack((np.zeros(len(train_x)),np.zeros(len(neg_train_x)),\n",
    "                           np.zeros(len(val_x)),np.zeros(len(neg_val_x)),\n",
    "                           np.ones(len(test_x))))\n",
    "    # normal training points\n",
    "    neighbor_mask = np.hstack((np.ones(len(train_x)), np.zeros(len(neg_train_x)),\n",
    "                               np.zeros(len(val_y)), np.zeros(len(neg_val_x)),\n",
    "                               np.zeros(len(test_y))))\n",
    "\n",
    "    # find k nearest neighbours (idx) and their distances (dist) to each points in x within neighbour_mask==1\n",
    "    dist, idx = find_neighbors(x, y, neighbor_mask, k)\n",
    "\n",
    "    return x.astype('float32'), y.astype('float32'), neighbor_mask.astype('float32'), train_mask.astype('float32'), val_mask.astype('float32'), test_mask.astype('float32'), dist, idx\n",
    "\n",
    "# loading negative samples\n",
    "def generate_negative_samples(x, sample_type, proportion, epsilon):\n",
    "\n",
    "    n_samples = int(proportion*(len(x)))\n",
    "    n_dim = x.shape[-1]\n",
    "\n",
    "    #M\n",
    "    randmat = np.random.rand(n_samples,n_dim) < 0.3\n",
    "    # uniform samples\n",
    "    rand_unif = (epsilon* (1-2*np.random.rand(n_samples,n_dim)))\n",
    "    #  subspace perturbation samples\n",
    "    rand_sub = np.tile(x, (proportion,1)) + randmat*(epsilon*np.random.randn(n_samples,n_dim))\n",
    "\n",
    "    if sample_type == 'UNIFORM':\n",
    "        neg_x = rand_unif\n",
    "    if sample_type == 'SUBSPACE':\n",
    "        neg_x = rand_sub\n",
    "    if sample_type == 'MIXED':\n",
    "        # randomly sample from uniform and gaussian negative samples\n",
    "        neg_x = np.concatenate((rand_unif, rand_sub),0)\n",
    "        neg_x = neg_x[np.random.choice(np.arange(len(neg_x)), size = n_samples)]\n",
    "\n",
    "    neg_y = np.ones(len(neg_x))\n",
    "\n",
    "    return neg_x.astype('float32'), neg_y.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iayk7CJxZTmP"
   },
   "source": [
    "GRAPH FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqwfA3ZRY3Eg"
   },
   "outputs": [],
   "source": [
    "# find the k nearest neighbours of all x points out of the neighbour candidates\n",
    "def find_neighbors(x, y, neighbor_mask, k):\n",
    "\n",
    "    # nearest neighbour object\n",
    "    index = faiss.IndexFlatL2(x.shape[-1])\n",
    "    # add nearest neighbour candidates\n",
    "    index.add(x[neighbor_mask==1])\n",
    "\n",
    "    # distances and idx of neighbour points for the neighbour candidates (k+1 as the first one will be the point itself)\n",
    "    dist_train, idx_train = index.search(x[neighbor_mask==1], k = k+1)\n",
    "    # remove 1st nearest neighbours to remove self loops\n",
    "    dist_train, idx_train = dist_train[:,1:], idx_train[:,1:]\n",
    "    # distances and idx of neighbour points for the non-neighbour candidates\n",
    "    dist_test, idx_test = index.search(x[neighbor_mask==0], k = k)\n",
    "    #concat\n",
    "    dist = np.vstack((dist_train, dist_test))\n",
    "    idx = np.vstack((idx_train, idx_test))\n",
    "\n",
    "    return dist, idx\n",
    "\n",
    "# create graph object out of x, y, distances and indices of neighbours\n",
    "def build_graph(x, y, dist, idx):\n",
    "\n",
    "    # array like [0,0,0,0,0,1,1,1,1,1,...,n,n,n,n,n] for k = 5 (i.e. edges sources)\n",
    "    idx_source = np.repeat(np.arange(len(x)),dist.shape[-1]).astype('int32')\n",
    "    idx_source = np.expand_dims(idx_source,axis=0)\n",
    "\n",
    "    # edge targets, i.e. the nearest k neighbours of point 0, 1,..., n\n",
    "    idx_target = idx.flatten()\n",
    "    idx_target = np.expand_dims(idx_target,axis=0).astype('int32')\n",
    "\n",
    "    #stack source and target indices\n",
    "    idx = np.vstack((idx_source, idx_target))\n",
    "\n",
    "    # edge weights\n",
    "    attr = dist.flatten()\n",
    "    attr = np.sqrt(attr)\n",
    "    attr = np.expand_dims(attr, axis=1)\n",
    "\n",
    "    # into tensors\n",
    "    x = torch.tensor(x, dtype = torch.float32)\n",
    "    y = torch.tensor(y,dtype = torch.float32)\n",
    "    idx = torch.tensor(idx, dtype = torch.long)\n",
    "    attr = torch.tensor(attr, dtype = torch.float32)\n",
    "\n",
    "    #build PyTorch geometric Data object\n",
    "    data = Data(x = x, edge_index = idx, edge_attr = attr, y = y)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cm41pHtcZWGn"
   },
   "source": [
    "DATASET FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCR2GGmmUwoM"
   },
   "outputs": [],
   "source": [
    "# split training data into train set and validation set\n",
    "def split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
    "    val_mask = np.zeros(len(all_train_x))\n",
    "    val_mask[val_idx] = 1\n",
    "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
    "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_x[train_y == 0])\n",
    "    train_x = scaler.transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "\n",
    "    if all_test_x is None:\n",
    "        test_x = val_x\n",
    "        test_y = val_y\n",
    "\n",
    "    test_x = scaler.transform(all_test_x)\n",
    "    test_y = all_test_y\n",
    "\n",
    "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
    "\n",
    "\n",
    "#load data\n",
    "def load_dataset(dataset,seed):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if dataset == 'MI-V':\n",
    "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
    "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "        normal_idx = np.ones(len(df))\n",
    "        for i in ['06','08','09','10']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
    "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
    "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
    "        data = df.to_numpy()\n",
    "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
    "        data = data[idx]\n",
    "        normal_idx = normal_idx[idx]\n",
    "        normal_data = data[normal_idx == 1]\n",
    "        anomaly_data = data[normal_idx == 0]\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "        train_x = normal_data[train_idx]\n",
    "        train_y = np.zeros(len(train_x))\n",
    "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "\n",
    "    elif dataset == 'MI-F':\n",
    "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
    "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "        normal_idx = np.ones(len(df))\n",
    "        for i in ['04', '05', '07', '16']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
    "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
    "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
    "        data = df.to_numpy()\n",
    "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
    "        data = data[idx]\n",
    "        normal_idx = normal_idx[idx]\n",
    "        normal_data = data[normal_idx == 1]\n",
    "        anomaly_data = data[normal_idx == 0]\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "        train_x = normal_data[train_idx]\n",
    "        train_y = np.zeros(len(train_x))\n",
    "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "\n",
    "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
    "        if dataset == 'SHUTTLE':\n",
    "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
    "        elif dataset == 'OPTDIGITS':\n",
    "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
    "        elif dataset == 'PENDIGITS':\n",
    "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
    "        label = data['y'].astype('float32').squeeze()\n",
    "        data = data['X'].astype('float32')\n",
    "        normal_data= data[label == 0]\n",
    "        normal_label = label[label==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "        train_x = normal_data[train_idx]\n",
    "        train_y = normal_label[train_idx]\n",
    "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "\n",
    "    elif dataset in ['THYROID','HRSS']:\n",
    "        if dataset == 'THYROID':\n",
    "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
    "        if dataset == 'HRSS':\n",
    "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
    "        label = data[:,-1].astype('float32').squeeze()\n",
    "        data = data[:,:-1].astype('float32')\n",
    "        normal_data= data[label == 0]\n",
    "        normal_label = label[label==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "        train_x = normal_data[train_idx]\n",
    "        train_y = normal_label[train_idx]\n",
    "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "\n",
    "    elif dataset == 'SATELLITE':\n",
    "        data = loadmat('data/SATELLITE/satellite.mat')\n",
    "        label = data['y'].astype('float32').squeeze()\n",
    "        data = data['X'].astype('float32')\n",
    "        normal_data = data[label == 0]\n",
    "        normal_label = label[label ==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
    "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
    "        train_x = normal_data[train_idx]\n",
    "        train_y = normal_label[train_idx]\n",
    "        test_x = normal_data[test_idx]\n",
    "        test_y = normal_label[test_idx]\n",
    "        test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
    "        test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
    "        test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
    "\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaD6MtKrZYYe"
   },
   "source": [
    "### lunar.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQVfnAaBU3ee"
   },
   "outputs": [],
   "source": [
    "# Message passing scheme\n",
    "class GNN1(MessagePassing):\n",
    "    def __init__(self,k):\n",
    "        super(GNN1, self).__init__(flow=\"target_to_source\")\n",
    "        self.k = k\n",
    "        self.hidden_size = 256\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(k,self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size,self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size,self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size,1),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        self.network = self.network.to(dtype = torch.float32)\n",
    "        out = self.propagate(edge_index = edge_index, x=x, edge_attr=edge_attr, k = self.k, network=self.network)\n",
    "        return out\n",
    "\n",
    "    def message(self,x_i,x_j,edge_attr):\n",
    "        # message is the edge weight\n",
    "        return edge_attr\n",
    "\n",
    "    def aggregate(self, inputs, index, k, network):\n",
    "        # concatenate all k messages\n",
    "        self.input_aggr = inputs.reshape(-1,k)\n",
    "        # pass through network\n",
    "        out = self.network(self.input_aggr)\n",
    "        return out\n",
    "\n",
    "# GNN\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, k):\n",
    "        super(GNN, self).__init__()\n",
    "        self.k = k\n",
    "        self.L1 = GNN1(self.k)\n",
    "    def forward(self,data):\n",
    "        self.edge_attr = data.edge_attr\n",
    "        self.edge_index = data.edge_index\n",
    "        self.x = data.x\n",
    "        out = self.L1(self.x, self.edge_index, self.edge_attr)\n",
    "        out = torch.squeeze(out,1)\n",
    "        return out\n",
    "\n",
    "def run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model):\n",
    "\n",
    "    # loss function\n",
    "    criterion = nn.MSELoss(reduction = 'none')\n",
    "\n",
    "    # path to save model parameters\n",
    "    model_path = 'saved_models/%s/%d/net_%d.pth' %(dataset,k,seed)\n",
    "    if not os.path.exists(os.path.dirname(model_path)):\n",
    "       os.makedirs(os.path.dirname(model_path))\n",
    "\n",
    "    x, y, neighbor_mask, train_mask, val_mask, test_mask, dist, idx = negative_samples(train_x, train_y, val_x, val_y, test_x, test_y, k, samples, proportion, epsilon)\n",
    "    data = build_graph(x, y, dist, idx)\n",
    "\n",
    "    data = data.to(device)\n",
    "    torch.manual_seed(seed)\n",
    "    net = GNN(k).to(device)\n",
    "\n",
    "    if train_new_model == True:\n",
    "\n",
    "        optimizer = optim.Adam(net.parameters(), lr = lr, weight_decay = wd)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            net.eval()\n",
    "            out = net(data)\n",
    "            loss = criterion(out,data.y)\n",
    "\n",
    "            val_loss = loss[val_mask == 1].mean()\n",
    "            val_score = roc_auc_score(data.y[val_mask==1].cpu(),out[val_mask==1].cpu())\n",
    "\n",
    "            best_val_score = 0\n",
    "\n",
    "        # training\n",
    "        for epoch in range(n_epochs):\n",
    "            net.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = net(data)\n",
    "            # loss for training data only\n",
    "            loss = criterion(out[train_mask == 1],data.y[train_mask == 1]).sum()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                net.eval()\n",
    "                out = net(data)\n",
    "                loss = criterion(out,data.y)\n",
    "\n",
    "                val_loss = loss[val_mask == 1].mean()\n",
    "                val_score = roc_auc_score(data.y[val_mask==1].cpu(),out[val_mask==1].cpu())\n",
    "\n",
    "                # if new model gives the best validation set score\n",
    "                if val_score >= best_val_score:\n",
    "\n",
    "                    # save model parameters\n",
    "                    best_dict = {'epoch': epoch,\n",
    "                           'model_state_dict': deepcopy(net.state_dict()),\n",
    "                           'optimizer_state_dict': deepcopy(optimizer.state_dict()),\n",
    "                           'val_loss': val_loss,\n",
    "                           'val_score': val_score,\n",
    "                           'k': k,}\n",
    "\n",
    "                    # save best model\n",
    "                    #torch.save(best_dict, model_path)\n",
    "\n",
    "                    # reset best score so far\n",
    "                    best_val_score = val_score\n",
    "\n",
    "        # load best model\n",
    "        net.load_state_dict(best_dict['model_state_dict'])\n",
    "\n",
    "    # if not training a new model, load the saved model\n",
    "    if train_new_model == False:\n",
    "\n",
    "        load_dict = torch.load(model_path)\n",
    "        net.load_state_dict(load_dict['model_state_dict'])\n",
    "\n",
    "    # testing\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        out = net(data)\n",
    "        loss = criterion(out,data.y)\n",
    "\n",
    "    # return output for test points\n",
    "    return out[test_mask==1].cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIePvIj5Zmwk"
   },
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iilwC0okTthg"
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    dataset = args.dataset\n",
    "    samples = args.samples\n",
    "    k = args.k\n",
    "    train_new_model = args.train_new_model\n",
    "    mode = args.mode\n",
    "\n",
    "    if mode == 0:\n",
    "        for seed in [0,4]:\n",
    "            #print(\"Running trial with random seed = %d\" %seed)\n",
    "            train_x, train_y, val_x, val_y, test_x, test_y = load_dataset(args.dataset,seed)\n",
    "            start = time.time()\n",
    "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,args.dataset,seed,args.k,args.samples,args.train_new_model)\n",
    "            end = time.time()\n",
    "            score = 100*roc_auc_score(test_y, test_out)\n",
    "            print('Dataset: %s \\t Samples: %s \\t k: %d \\t Score: %.4f \\t Runtime: %.2f seconds' %(args.dataset,args.samples,args.k,score,(end-start)))\n",
    "    elif mode == 1:\n",
    "        Ps = [10, 30, 50]\n",
    "        for P in Ps:\n",
    "            print(f\"Percentage = {P} : \")\n",
    "            seed = 3\n",
    "            #print(\"Running trial with random seed = %d\" %seed)\n",
    "            train_x, train_y, val_x, val_y, test_x, test_y = target1_load_dataset(args.dataset,seed, P)\n",
    "            start = time.time()\n",
    "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,args.dataset,seed,args.k,args.samples,args.train_new_model)\n",
    "            end = time.time()\n",
    "            score = 100*roc_auc_score(test_y, test_out)\n",
    "            print('Dataset: %s \\t Samples: %s \\t k: %d \\t Score: %.4f \\t Runtime: %.2f seconds' %(args.dataset,args.samples,args.k,score,(end-start)))\n",
    "    elif mode == 2:\n",
    "        percentages = [50, 70, 90]\n",
    "        for percentage in percentages:\n",
    "            print(f\"Percentage = {percentage} : \")\n",
    "            seed = 3\n",
    "            #print(\"Running trial with random seed = %d\" %seed)\n",
    "            train_x, train_y, val_x, val_y, test_x, test_y = target2_load_dataset(args.dataset,seed,percentage)\n",
    "            start = time.time()\n",
    "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,args.dataset,seed,args.k,args.samples,args.train_new_model)\n",
    "            end = time.time()\n",
    "            score = 100*roc_auc_score(test_y, test_out)\n",
    "            print('Dataset: %s \\t Samples: %s \\t k: %d \\t Score: %.4f \\t Runtime: %.2f seconds' %(args.dataset,args.samples,args.k,score,(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tDK2c7oVBlZ"
   },
   "source": [
    "# experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l9GxNi80-3y"
   },
   "source": [
    "### Target 1\n",
    "Decreasing performance by training on noisy data: add a proportion P% of anomalies into the training set (change P%=10%,30%,50%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5lR3XiKl1BZa"
   },
   "outputs": [],
   "source": [
    "def target1_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
    "    val_mask = np.zeros(len(all_train_x))\n",
    "    val_mask[val_idx] = 1\n",
    "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
    "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_x[train_y == 0])\n",
    "    train_x = scaler.transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "\n",
    "    if all_test_x is None:\n",
    "        test_x = val_x\n",
    "        test_y = val_y\n",
    "\n",
    "    test_x = scaler.transform(all_test_x)\n",
    "    test_y = all_test_y\n",
    "\n",
    "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
    "\n",
    "\n",
    "#load data\n",
    "def target1_load_dataset(dataset,seed, P):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if dataset == 'MI-V':\n",
    "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
    "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "        normal_idx = np.ones(len(df))\n",
    "        for i in ['06','08','09','10']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
    "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
    "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
    "        data = df.to_numpy()\n",
    "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
    "        data = data[idx]\n",
    "        normal_idx = normal_idx[idx]\n",
    "        normal_data = data[normal_idx == 1]\n",
    "        anomaly_data = data[normal_idx == 0]\n",
    "\n",
    "        num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
    "        num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
    "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
    "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
    "\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "        train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
    "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "        test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
    "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
    "\n",
    "        #train_x = normal_data[train_idx]\n",
    "        #train_y = np.zeros(len(train_x))\n",
    "        #test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "        #test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "\n",
    "    elif dataset == 'MI-F':\n",
    "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
    "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "        normal_idx = np.ones(len(df))\n",
    "        for i in ['04', '05', '07', '16']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
    "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
    "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
    "        data = df.to_numpy()\n",
    "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
    "        data = data[idx]\n",
    "        normal_idx = normal_idx[idx]\n",
    "        normal_data = data[normal_idx == 1]\n",
    "        anomaly_data = data[normal_idx == 0]\n",
    "\n",
    "        num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
    "        num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
    "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
    "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
    "\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "        train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
    "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "        test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
    "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
    "\n",
    "        #train_x = normal_data[train_idx]\n",
    "        #train_y = np.zeros(len(train_x))\n",
    "        #test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "        #test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "\n",
    "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
    "        if dataset == 'SHUTTLE':\n",
    "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
    "        elif dataset == 'OPTDIGITS':\n",
    "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
    "        elif dataset == 'PENDIGITS':\n",
    "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
    "        label = data['y'].astype('float32').squeeze()\n",
    "        data = data['X'].astype('float32')\n",
    "        normal_data= data[label == 0]\n",
    "        normal_label = label[label==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "\n",
    "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
    "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
    "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
    "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
    "\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
    "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "        test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
    "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
    "\n",
    "        #train_x = normal_data[train_idx]\n",
    "        #train_y = normal_label[train_idx]\n",
    "        #test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "        #test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "\n",
    "    elif dataset in ['THYROID','HRSS']:\n",
    "        if dataset == 'THYROID':\n",
    "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
    "        if dataset == 'HRSS':\n",
    "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
    "        label = data[:,-1].astype('float32').squeeze()\n",
    "        data = data[:,:-1].astype('float32')\n",
    "        normal_data= data[label == 0]\n",
    "        normal_label = label[label==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "\n",
    "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
    "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
    "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
    "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
    "\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
    "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "        test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
    "        test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
    "\n",
    "        #train_x = normal_data[train_idx]\n",
    "        #train_y = normal_label[train_idx]\n",
    "        #test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "        #test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "\n",
    "    elif dataset == 'SATELLITE':\n",
    "        data = loadmat('data/SATELLITE/satellite.mat')\n",
    "        label = data['y'].astype('float32').squeeze()\n",
    "        data = data['X'].astype('float32')\n",
    "        normal_data = data[label == 0]\n",
    "        normal_label = label[label ==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "\n",
    "        num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
    "        num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
    "        anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
    "        remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
    "\n",
    "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
    "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
    "\n",
    "        train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
    "        train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "        test_x = normal_data[test_idx]\n",
    "        test_y  = normal_label[test_idx]\n",
    "\n",
    "        selected_anomaly_idx = np.random.choice(remaining_anomaly_idx, len(test_x), replace=False)\n",
    "        test_x = np.concatenate((test_x, anom_data[selected_anomaly_idx]))\n",
    "        test_y = np.concatenate((test_y, anom_label[selected_anomaly_idx]))\n",
    "\n",
    "        #train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
    "        #test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
    "        #train_x = normal_data[train_idx]\n",
    "        #train_y = normal_label[train_idx]\n",
    "        #test_x = normal_data[test_idx]\n",
    "        #test_y = normal_label[test_idx]\n",
    "        #test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
    "        #test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
    "        #test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
    "\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = target1_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObEhMJiA2V2f"
   },
   "source": [
    "### Target 2\n",
    "decreasing performance by reduce the number of training data: reduce the number of normal samples in the training set (change to 50%, 70%, 90% of the original normal samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iS6j9TgQ3b_o"
   },
   "outputs": [],
   "source": [
    "def reduce_normal_samples(normal_data, percentage):\n",
    "    # Calculate the number of normal samples to keep based on the percentage\n",
    "    num_samples_to_keep = int(len(normal_data) * percentage / 100)\n",
    "    # Randomly select\n",
    "    idx_to_keep = np.random.choice(np.arange(0, len(normal_data)), num_samples_to_keep, replace=False)\n",
    "    return normal_data[idx_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K0LAZYniQRTQ"
   },
   "outputs": [],
   "source": [
    "def target2_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    val_idx = np.random.choice(np.arange(len(all_train_x)), size=int(0.15 * len(all_train_x)), replace=False)\n",
    "    val_mask = np.zeros(len(all_train_x), dtype=bool)\n",
    "    val_mask[val_idx] = True\n",
    "\n",
    "    # Use np.where to split data based on the boolean mask\n",
    "    val_x, val_y = all_train_x[np.where(val_mask)], all_train_y[np.where(val_mask)]\n",
    "    train_x, train_y = all_train_x[np.where(~val_mask)], all_train_y[np.where(~val_mask)]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_x[train_y == 0])\n",
    "    train_x = scaler.transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "\n",
    "    if all_test_x is None:\n",
    "        test_x = val_x\n",
    "        test_y = val_y\n",
    "    else:\n",
    "        test_x = scaler.transform(all_test_x)\n",
    "        test_y = all_test_y\n",
    "\n",
    "    return (\n",
    "        train_x.astype('float32'), train_y.astype('float32'),\n",
    "        val_x.astype('float32'), val_y.astype('float32'),\n",
    "        test_x.astype('float32'), test_y.astype('float32')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_SQR3Isy2iER"
   },
   "outputs": [],
   "source": [
    "def target2_load_dataset(dataset,seed,percentage):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if dataset == 'MI-V':\n",
    "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
    "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "        normal_idx = np.ones(len(df))\n",
    "        for i in ['06','08','09','10']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
    "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
    "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
    "        data = df.to_numpy()\n",
    "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
    "        data = data[idx]\n",
    "        normal_idx = normal_idx[idx]\n",
    "        normal_data = data[normal_idx == 1]\n",
    "        anomaly_data = data[normal_idx == 0]\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "\n",
    "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
    "        #train_x = normal_data[train_idx]\n",
    "        train_y = np.zeros(len(train_x))\n",
    "\n",
    "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "\n",
    "    elif dataset == 'MI-F':\n",
    "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
    "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "        normal_idx = np.ones(len(df))\n",
    "        for i in ['04', '05', '07', '16']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
    "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
    "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
    "        data = df.to_numpy()\n",
    "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
    "        data = data[idx]\n",
    "        normal_idx = normal_idx[idx]\n",
    "        normal_data = data[normal_idx == 1]\n",
    "        anomaly_data = data[normal_idx == 0]\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "\n",
    "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
    "        #train_x = normal_data[train_idx]\n",
    "        train_y = np.zeros(len(train_x))\n",
    "\n",
    "        test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "        test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "\n",
    "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
    "        if dataset == 'SHUTTLE':\n",
    "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
    "        elif dataset == 'OPTDIGITS':\n",
    "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
    "        elif dataset == 'PENDIGITS':\n",
    "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
    "        label = data['y'].astype('float32').squeeze()\n",
    "        data = data['X'].astype('float32')\n",
    "        normal_data= data[label == 0]\n",
    "        normal_label = label[label==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "\n",
    "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
    "        #train_x = normal_data[train_idx]\n",
    "        train_y = normal_label[train_idx]\n",
    "\n",
    "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "\n",
    "    elif dataset in ['THYROID','HRSS']:\n",
    "        if dataset == 'THYROID':\n",
    "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
    "        if dataset == 'HRSS':\n",
    "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
    "        label = data[:,-1].astype('float32').squeeze()\n",
    "        data = data[:,:-1].astype('float32')\n",
    "        normal_data= data[label == 0]\n",
    "        normal_label = label[label==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "        test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
    "        train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "\n",
    "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
    "        #train_x = normal_data[train_idx]\n",
    "        train_y = normal_label[train_idx]\n",
    "\n",
    "        test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "        test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "\n",
    "    elif dataset == 'SATELLITE':\n",
    "        data = loadmat('data/SATELLITE/satellite.mat')\n",
    "        label = data['y'].astype('float32').squeeze()\n",
    "        data = data['X'].astype('float32')\n",
    "        normal_data = data[label == 0]\n",
    "        normal_label = label[label ==0]\n",
    "        anom_data = data[label == 1]\n",
    "        anom_label = label[label ==1]\n",
    "        train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
    "        test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
    "\n",
    "        train_x = reduce_normal_samples(normal_data[train_idx], percentage)\n",
    "        #train_x = normal_data[train_idx]\n",
    "        train_y = normal_label[train_idx]\n",
    "\n",
    "        test_x = normal_data[test_idx]\n",
    "        test_y = normal_label[test_idx]\n",
    "        test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
    "        test_x = np.concatenate((test_x,anom_data[test_idx]))\n",
    "        test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
    "\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = target2_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
    "\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ZRbahrz2cXK"
   },
   "source": [
    "### Target 3\n",
    "random subsetting the features => K feature subsets cover all features (e.g., K=3,6,9), then construct K LUNAR models => average the scores of K models to be final outcome\n",
    "\n",
    "HRSS : 20 | MI-F : 58 | MI-V : 58 | OPTDIGITS : 64  \n",
    "PENDIGITS : 16 | SATELLITE : 36 | SHUTTLE : 9 | THYROIDm : 21  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNFToJ6tbZGT"
   },
   "outputs": [],
   "source": [
    "def generate_mask2(total_features, subset_size, overlap_percentage, selected):\n",
    "    #First: randomly selected 1 / K (subset_size) features (from selected == 0) -> selected_indices\n",
    "    mask = np.zeros(total_features, dtype=bool)\n",
    "    selected_indices = np.random.choice(np.where(selected == 0)[0], subset_size, replace=False)\n",
    "    #Second: Calculate num_overlap , choose another num_overlap features by np.setdiff1d using selected_indices -> overlap_indices\n",
    "    num_overlap = int((total_features - subset_size) * overlap_percentage)\n",
    "    overlap_indices = np.random.choice(np.setdiff1d(np.arange(total_features), selected_indices), num_overlap, replace=False)\n",
    "    #Third: update selected (where selected_indices) to 1\n",
    "    selected[selected_indices] = 1\n",
    "    #Fourth: add overlap_indices & selected_indices to mask\n",
    "    mask[overlap_indices] = True\n",
    "    mask[selected_indices] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "06mTSzTAKZAd"
   },
   "outputs": [],
   "source": [
    "def generate_mask1(total_features, subset_size, overlap_percentage):\n",
    "    mask = np.zeros(total_features, dtype=bool)\n",
    "    num_selected = int(subset_size + (total_features - subset_size) * overlap_percentage)\n",
    "    selected_indices = np.random.choice(total_features, num_selected, replace=False)\n",
    "    mask[selected_indices] = True\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0T3yHiEVTV8A"
   },
   "outputs": [],
   "source": [
    "# split training data into train set and validation set\n",
    "def target3_split_data(seed, all_train_x, all_train_y, all_test_x, all_test_y):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    val_idx = np.random.choice(np.arange(len(all_train_x)),size = int(0.15*len(all_train_x)), replace = False)\n",
    "    val_mask = np.zeros(len(all_train_x))\n",
    "    val_mask[val_idx] = 1\n",
    "    val_x = all_train_x[val_mask == 1]; val_y = all_train_y[val_mask == 1]\n",
    "    train_x = all_train_x[val_mask == 0]; train_y = all_train_y[val_mask == 0]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_x[train_y == 0])\n",
    "    train_x = scaler.transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "\n",
    "    if all_test_x is None:\n",
    "        test_x = val_x\n",
    "        test_y = val_y\n",
    "\n",
    "    test_x = scaler.transform(all_test_x)\n",
    "    test_y = all_test_y\n",
    "\n",
    "    return train_x.astype('float32'), train_y.astype('float32'), val_x.astype('float32'), val_y.astype('float32'),  test_x.astype('float32'), test_y.astype('float32')\n",
    "\n",
    "#load data\n",
    "def target3_load_dataset(dataset,seed,k,samples,train_new_model, K, overlap, mode, P):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if dataset == 'MI-V':\n",
    "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
    "        for i in ['02','03','11','12','13','14','15','17','18']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "        normal_idx = np.ones(len(df))\n",
    "        for i in ['06','08','09','10']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
    "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
    "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
    "        data = df.to_numpy()\n",
    "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
    "        data = data[idx]\n",
    "        normal_idx = normal_idx[idx]\n",
    "\n",
    "        total_features = data.shape[1]\n",
    "        subset_size = int(total_features / K)\n",
    "        test_out_list = []\n",
    "        selected = np.zeros(total_features, dtype=bool)\n",
    "        for i in range(K):\n",
    "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
    "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
    "            #print(f\"Selected Features for iteration {i + 1}: {np.where(mask)[0]}\")\n",
    "            subset_data = data[:, mask]\n",
    "\n",
    "            normal_data = subset_data[normal_idx == 1]\n",
    "            anomaly_data = subset_data[normal_idx == 0]\n",
    "            if mode == 0:\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = normal_data[train_idx]\n",
    "                train_y = np.zeros(len(train_x))\n",
    "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "            elif mode == 1:\n",
    "                num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
    "                num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
    "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
    "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
    "\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
    "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "                test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
    "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
    "            elif mode == 2:\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
    "                train_y = np.zeros(len(train_x))\n",
    "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "\n",
    "            train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
    "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
    "\n",
    "            test_out_list.append(test_out)\n",
    "\n",
    "    elif dataset == 'MI-F':\n",
    "        df = pd.read_csv(\"data/MI/experiment_01.csv\")\n",
    "        for i in ['02','03','06','08','09','10','11','12','13','14','15','17','18']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "        normal_idx = np.ones(len(df))\n",
    "        for i in ['04', '05', '07', '16']:\n",
    "            data = pd.read_csv(\"data/MI/experiment_%s.csv\" %i)\n",
    "            #df = df.append(data, ignore_index = True)\n",
    "            df = pd.concat([df,data], ignore_index = True)\n",
    "            normal_idx = np.append(normal_idx,np.zeros(len(data)))\n",
    "        machining_process_one_hot = pd.get_dummies(df['Machining_Process'])\n",
    "        df = pd.concat([df.drop(['Machining_Process'],axis=1),machining_process_one_hot],axis=1)\n",
    "        data = df.to_numpy()\n",
    "        idx = np.unique(data,axis=0, return_index = True)[1]\n",
    "        data = data[idx]\n",
    "        normal_idx = normal_idx[idx]\n",
    "\n",
    "        total_features = data.shape[1]\n",
    "        subset_size = int(total_features / K)\n",
    "        test_out_list = []\n",
    "        selected = np.zeros(total_features, dtype=bool)\n",
    "        for i in range(K):\n",
    "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
    "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
    "            #print(f\"Selected Features for iteration {i + 1}: {np.where(mask)[0]}\")\n",
    "            subset_data = data[:, mask]\n",
    "\n",
    "            normal_data = subset_data[normal_idx == 1]\n",
    "            anomaly_data = subset_data[normal_idx == 0]\n",
    "            if mode == 0:\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = normal_data[train_idx]\n",
    "                train_y = np.zeros(len(train_x))\n",
    "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "            elif mode == 1:\n",
    "                num_anomalies_to_add = int(len(anomaly_data) * P / 100)\n",
    "                num_anomalies_remain = len(anomaly_data) - num_anomalies_to_add\n",
    "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anomaly_data)), num_anomalies_to_add, replace=False)\n",
    "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anomaly_data)), anomaly_idx_to_add)\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = np.concatenate((normal_data[train_idx], anomaly_data[anomaly_idx_to_add]))\n",
    "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "                test_x = np.concatenate((anomaly_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
    "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
    "            elif mode == 2:\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anomaly_data), replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
    "                train_y = np.zeros(len(train_x))\n",
    "                test_x = np.concatenate((anomaly_data,normal_data[test_idx]))\n",
    "                test_y  = np.concatenate((np.ones(len(anomaly_data)),np.zeros(len(test_idx))))\n",
    "\n",
    "            train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
    "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
    "\n",
    "            test_out_list.append(test_out)\n",
    "\n",
    "    elif dataset in ['OPTDIGITS', 'PENDIGITS','SHUTTLE']:\n",
    "        if dataset == 'SHUTTLE':\n",
    "            data = loadmat(\"data/SHUTTLE/shuttle.mat\")\n",
    "        elif dataset == 'OPTDIGITS':\n",
    "            data = loadmat(\"data/optdigits/optdigits.mat\")\n",
    "        elif dataset == 'PENDIGITS':\n",
    "            data = loadmat('data/PENDIGITS/pendigits.mat')\n",
    "        label = data['y'].astype('float32').squeeze()\n",
    "        data = data['X'].astype('float32')\n",
    "\n",
    "        total_features = data.shape[1]\n",
    "        subset_size = int(total_features / K)\n",
    "        test_out_list = []\n",
    "        selected = np.zeros(total_features, dtype=bool)\n",
    "        for i in range(K):\n",
    "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
    "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
    "            #print(f\"Selected Features for iteration {i + 1}: {np.where(mask)[0]}\")\n",
    "            subset_data = data[:, mask]\n",
    "\n",
    "            normal_data= subset_data[label == 0]\n",
    "            normal_label = label[label==0]\n",
    "            anom_data = subset_data[label == 1]\n",
    "            anom_label = label[label ==1]\n",
    "            if mode == 0:\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = normal_data[train_idx]\n",
    "                train_y = normal_label[train_idx]\n",
    "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "            elif mode == 1:\n",
    "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
    "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
    "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
    "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
    "\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
    "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "                test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
    "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
    "            elif mode == 2:\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
    "                train_y = normal_label[train_idx]\n",
    "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "\n",
    "            train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
    "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
    "\n",
    "            test_out_list.append(test_out)\n",
    "\n",
    "    elif dataset in ['THYROID','HRSS']:\n",
    "        if dataset == 'THYROID':\n",
    "            data = pd.read_csv('data/THYROID/annthyroid_21feat_normalised.csv').to_numpy()\n",
    "        if dataset == 'HRSS':\n",
    "            data = pd.read_csv('data/HRSS/HRSS.csv').to_numpy()\n",
    "        label = data[:,-1].astype('float32').squeeze()\n",
    "        data = data[:,:-1].astype('float32')\n",
    "\n",
    "        total_features = data.shape[1]\n",
    "        subset_size = int(total_features / K)\n",
    "        test_out_list = []\n",
    "        selected = np.zeros(total_features, dtype=bool)\n",
    "        for i in range(K):\n",
    "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
    "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
    "            #print(f\"Selected Features for iteration {i + 1}: {np.where(mask)[0]}\")\n",
    "            subset_data = data[:, mask]\n",
    "\n",
    "            normal_data= subset_data[label == 0]\n",
    "            normal_label = label[label==0]\n",
    "            anom_data = subset_data[label == 1]\n",
    "            anom_label = label[label ==1]\n",
    "            if mode == 0:\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = normal_data[train_idx]\n",
    "                train_y = normal_label[train_idx]\n",
    "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "            elif mode == 1:\n",
    "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
    "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
    "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
    "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
    "\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), num_anomalies_remain, replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
    "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "                test_x = np.concatenate((anom_data[remaining_anomaly_idx], normal_data[test_idx]))\n",
    "                test_y  = np.concatenate((np.ones(len(remaining_anomaly_idx)), np.zeros(len(test_idx))))\n",
    "            elif mode == 2:\n",
    "                test_idx = np.random.choice(np.arange(0,len(normal_data)), len(anom_data), replace = False)\n",
    "                train_idx = np.setdiff1d(np.arange(0,len(normal_data)), test_idx)\n",
    "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
    "                train_y = normal_label[train_idx]\n",
    "                test_x = np.concatenate((normal_data[test_idx],anom_data))\n",
    "                test_y = np.concatenate((normal_label[test_idx],anom_label))\n",
    "            train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
    "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
    "\n",
    "            test_out_list.append(test_out)\n",
    "\n",
    "    elif dataset == 'SATELLITE':\n",
    "        data = loadmat('data/SATELLITE/satellite.mat')\n",
    "        label = data['y'].astype('float32').squeeze()\n",
    "        data = data['X'].astype('float32')\n",
    "\n",
    "        total_features = data.shape[1]\n",
    "        subset_size = int(total_features / K)\n",
    "        test_out_list = []\n",
    "        selected = np.zeros(total_features, dtype=bool)\n",
    "        for i in range(K):\n",
    "            #mask = generate_mask1(total_features, subset_size, overlap)\n",
    "            mask = generate_mask2(total_features, subset_size, overlap, selected)\n",
    "            #print(f\"Selected Features for iteration {i + 1}: {np.where(mask)[0]}\")\n",
    "            subset_data = data[:, mask]\n",
    "\n",
    "            normal_data= subset_data[label == 0]\n",
    "            normal_label = label[label==0]\n",
    "            anom_data = subset_data[label == 1]\n",
    "            anom_label = label[label ==1]\n",
    "            if mode == 0:\n",
    "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
    "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
    "                train_x = normal_data[train_idx]\n",
    "                train_y = normal_label[train_idx]\n",
    "                test_x = normal_data[test_idx]\n",
    "                test_y = normal_label[test_idx]\n",
    "                test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
    "                test_x = np.concatenate((test_x, anom_data[test_idx]))\n",
    "                test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
    "            elif mode == 1:\n",
    "                num_anomalies_to_add = int(len(anom_data) * P / 100)\n",
    "                num_anomalies_remain = len(anom_data) - num_anomalies_to_add\n",
    "                anomaly_idx_to_add = np.random.choice(np.arange(0, len(anom_data)), num_anomalies_to_add, replace=False)\n",
    "                remaining_anomaly_idx = np.setdiff1d(np.arange(0, len(anom_data)), anomaly_idx_to_add)\n",
    "\n",
    "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
    "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
    "\n",
    "                train_x = np.concatenate((normal_data[train_idx], anom_data[anomaly_idx_to_add]))\n",
    "                train_y = np.concatenate((np.zeros(len(train_idx)), np.zeros(len(anomaly_idx_to_add))))\n",
    "                test_x = normal_data[test_idx]\n",
    "                test_y  = normal_label[test_idx]\n",
    "\n",
    "                selected_anomaly_idx = np.random.choice(remaining_anomaly_idx, len(test_x), replace=False)\n",
    "                test_x = np.concatenate((test_x, anom_data[selected_anomaly_idx]))\n",
    "                test_y = np.concatenate((test_y, anom_label[selected_anomaly_idx]))\n",
    "            elif mode == 2:\n",
    "                train_idx = np.random.choice(np.arange(0,len(normal_data)), 4000, replace = False)\n",
    "                test_idx = np.setdiff1d(np.arange(0,len(normal_data)), train_idx)\n",
    "                train_x = reduce_normal_samples(normal_data[train_idx], P)\n",
    "                train_y = normal_label[train_idx]\n",
    "                test_x = normal_data[test_idx]\n",
    "                test_y = normal_label[test_idx]\n",
    "                test_idx = np.random.choice(np.arange(0,len(anom_data)), int(len(test_x)), replace = False)\n",
    "                test_x = np.concatenate((test_x, anom_data[test_idx]))\n",
    "                test_y = np.concatenate((test_y, anom_label[test_idx]))\n",
    "\n",
    "            train_x, train_y, val_x, val_y, test_x, test_y = target3_split_data(seed, all_train_x = train_x, all_train_y = train_y, all_test_x = test_x, all_test_y = test_y)\n",
    "            test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,seed,k,samples,train_new_model)\n",
    "\n",
    "            test_out_list.append(test_out)\n",
    "\n",
    "    #final_out = torch.where(torch.mean(torch.stack(test_out_list), dim=0) > 0.5, 1, 0).numpy()\n",
    "    final_out = torch.mean(torch.stack(test_out_list), dim=0).numpy()\n",
    "    #score = 100*roc_auc_score(test_y, test_out)\n",
    "    score = 100*roc_auc_score(test_y, final_out)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa7iygN3VK3u"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuJGRP8mKNAI",
    "outputId": "c273f866-18e8-4234-cf34-e332031f51c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap : 0.9 ------------------------------------------------------------------------------------------\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 70.1203 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 58.6848 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 80.9339 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 74.5674 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 93.4460 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 79.8829 \t\n",
      "Overlap : 1 ------------------------------------------------------------------------------------------\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 72.1140 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 48.4263 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 90.7603 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 67.8293 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 94.3173 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 82.1971 \t\n"
     ]
    }
   ],
   "source": [
    "#datasets = [\"HRSS\", \"MI-F\", \"MI-V\", \"OPTDIGITS\", \"PENDIGITS\", \"SATELLITE\", \"SHUTTLE\", \"THYROID\"]\n",
    "datasets = [\"MI-V\", \"OPTDIGITS\", \"PENDIGITS\", \"SATELLITE\", \"SHUTTLE\", \"THYROID\"]\n",
    "#modes = [0, 1, 2]\n",
    "mode = 1\n",
    "P = 30\n",
    "\n",
    "K = 9\n",
    "#overlaps = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "overlaps = [0.9, 1]\n",
    "\n",
    "#overlap_percentage_values = [0.1, 0.2, 0.3]\n",
    "#overlap_percentages = {dataset: overlap_percentage for dataset, overlap_percentage in zip(datasets, overlap_percentage_values)}\n",
    "#    overlap = overlap_percentages[dataset]\n",
    "\n",
    "k = 100\n",
    "train_new_model = True\n",
    "samples = 'MIXED'\n",
    "\n",
    "for overlap in overlaps:\n",
    "  print(f\"Overlap : {overlap} ------------------------------------------------------------------------------------------\")\n",
    "  for dataset in datasets:\n",
    "      print(f\"Dataset: {dataset}\")\n",
    "      train_x, train_y, val_x, val_y, test_x, test_y = load_dataset(dataset,0)\n",
    "      test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,0,k,samples,True)\n",
    "      origin_score = 100 * roc_auc_score(test_y, test_out)\n",
    "      scores = []\n",
    "      for seed in [0]:\n",
    "          score = target3_load_dataset(dataset,seed,k,samples,train_new_model, K, overlap, mode, P)\n",
    "          scores.append(score)\n",
    "      average_score = np.mean(scores)\n",
    "      print('LUNAR : Score: %.4f \\t' %(origin_score))\n",
    "      print('Subset : Score: %.4f \\t' %(average_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "09I9eU1iCSY3",
    "outputId": "63472ee3-b0ea-4808-82da-a1649ab1a512"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap : 0.1 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 67.5409 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 40.7041 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 61.4011 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 55.3956 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 56.9855 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 57.1140 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 58.6244 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 84.7761 \t\n",
      "Overlap : 0.2 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 69.0334 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 42.2086 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 62.0968 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 53.2800 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 39.8915 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 61.3288 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 73.6807 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 73.1319 \t\n",
      "Overlap : 0.3 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 74.3912 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 39.9792 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 66.0506 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 46.2222 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 38.3794 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 61.2615 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 73.6807 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 87.6980 \t\n",
      "Overlap : 0.4 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 70.5782 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 41.0191 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 61.4562 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 47.0578 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 39.5135 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 53.1435 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 94.3454 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 85.2067 \t\n",
      "Overlap : 0.5 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 73.0827 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 43.9764 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 64.1907 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 40.4267 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 53.4188 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 52.6467 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 85.0855 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 80.4430 \t\n",
      "Overlap : 0.6 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 69.4550 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 39.4440 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 60.0474 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 42.6311 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 31.8376 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 55.1096 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 85.0855 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 86.0469 \t\n",
      "Overlap : 0.7 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 74.7942 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 41.2158 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 63.0067 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 40.1067 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 60.0263 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 50.7321 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 75.2209 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 77.5000 \t\n",
      "Overlap : 0.8 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 75.4814 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 43.6236 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 66.3334 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 28.9067 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 34.0730 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 57.9726 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 75.2209 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 73.5794 \t\n",
      "Overlap : 0.9 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n",
      "LUNAR : Score: 92.5229 \t\n",
      "Subset : Score: 80.3368 \t\n",
      "Dataset: MI-F\n",
      "LUNAR : Score: 83.8371 \t\n",
      "Subset : Score: 39.9966 \t\n",
      "Dataset: MI-V\n",
      "LUNAR : Score: 96.7758 \t\n",
      "Subset : Score: 66.2653 \t\n",
      "Dataset: OPTDIGITS\n",
      "LUNAR : Score: 99.5067 \t\n",
      "Subset : Score: 29.4400 \t\n",
      "Dataset: PENDIGITS\n",
      "LUNAR : Score: 99.9959 \t\n",
      "Subset : Score: 44.7732 \t\n",
      "Dataset: SATELLITE\n",
      "LUNAR : Score: 87.0101 \t\n",
      "Subset : Score: 54.8929 \t\n",
      "Dataset: SHUTTLE\n",
      "LUNAR : Score: 99.9677 \t\n",
      "Subset : Score: 85.9532 \t\n",
      "Dataset: THYROID\n",
      "LUNAR : Score: 83.4089 \t\n",
      "Subset : Score: 78.3753 \t\n",
      "Overlap : 1 ------------------------------------------------------------------------------------------\n",
      "Dataset: HRSS\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-91f0831bcaf5>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset: {dataset}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m       \u001b[0mtest_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0morigin_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-a0831cbd4e23>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(train_x, train_y, val_x, val_y, test_x, test_y, dataset, seed, k, samples, train_new_model)\u001b[0m\n\u001b[1;32m     56\u001b[0m        \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproportion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3b1d3781b553>\u001b[0m in \u001b[0;36mnegative_samples\u001b[0;34m(train_x, train_y, val_x, val_y, test_x, test_y, k, sample_type, proportion, epsilon)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# find k nearest neighbours (idx) and their distances (dist) to each points in x within neighbour_mask==1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneighbor_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-514084faf00f>\u001b[0m in \u001b[0;36mfind_neighbors\u001b[0;34m(x, y, neighbor_mask, k)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# distances and idx of neighbour points for the neighbour candidates (k+1 as the first one will be the point itself)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdist_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneighbor_mask\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# remove 1st nearest neighbours to remove self loops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdist_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/faiss/__init__.py\u001b[0m in \u001b[0;36mreplacement_search\u001b[0;34m(self, x, k, D, I)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, n, x, k, distances, labels)\u001b[0m\n\u001b[1;32m   2144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFlat_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrange_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = [\"HRSS\", \"MI-F\", \"MI-V\", \"OPTDIGITS\", \"PENDIGITS\", \"SATELLITE\", \"SHUTTLE\", \"THYROID\"]\n",
    "\n",
    "#modes = [0, 1, 2]\n",
    "mode = 1\n",
    "P = 50\n",
    "\n",
    "K = 3\n",
    "overlaps = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "#overlap_percentage_values = [0.1, 0.2, 0.3]\n",
    "#overlap_percentages = {dataset: overlap_percentage for dataset, overlap_percentage in zip(datasets, overlap_percentage_values)}\n",
    "#    overlap = overlap_percentages[dataset]\n",
    "\n",
    "k = 100\n",
    "train_new_model = True\n",
    "samples = 'MIXED'\n",
    "\n",
    "for overlap in overlaps:\n",
    "  print(f\"Overlap : {overlap} ------------------------------------------------------------------------------------------\")\n",
    "  for dataset in datasets:\n",
    "      print(f\"Dataset: {dataset}\")\n",
    "      train_x, train_y, val_x, val_y, test_x, test_y = load_dataset(dataset,0)\n",
    "      test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,0,k,samples,True)\n",
    "      origin_score = 100 * roc_auc_score(test_y, test_out)\n",
    "      scores = []\n",
    "      for seed in [0]:\n",
    "          score = target3_load_dataset(dataset,seed,k,samples,train_new_model, K, overlap, mode, P)\n",
    "          scores.append(score)\n",
    "      average_score = np.mean(scores)\n",
    "      print('LUNAR : Score: %.4f \\t' %(origin_score))\n",
    "      print('Subset : Score: %.4f \\t' %(average_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOvY4wVVjGPz"
   },
   "outputs": [],
   "source": [
    "datasets = [\"HRSS\", \"MI-F\", \"MI-V\", \"OPTDIGITS\", \"PENDIGITS\", \"SATELLITE\", \"SHUTTLE\", \"THYROID\"]\n",
    "#modes = [0, 1, 2]\n",
    "mode = 1\n",
    "P = 50\n",
    "\n",
    "K = 6\n",
    "overlaps = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "\n",
    "#overlap_percentage_values = [0.1, 0.2, 0.3]\n",
    "#overlap_percentages = {dataset: overlap_percentage for dataset, overlap_percentage in zip(datasets, overlap_percentage_values)}\n",
    "#    overlap = overlap_percentages[dataset]\n",
    "\n",
    "k = 100\n",
    "train_new_model = True\n",
    "samples = 'MIXED'\n",
    "\n",
    "for overlap in overlaps:\n",
    "  print(f\"Overlap : {overlap} ------------------------------------------------------------------------------------------\")\n",
    "  for dataset in datasets:\n",
    "      print(f\"Dataset: {dataset}\")\n",
    "      train_x, train_y, val_x, val_y, test_x, test_y = load_dataset(dataset,0)\n",
    "      test_out = run(train_x,train_y,val_x,val_y,test_x,test_y,dataset,0,k,samples,True)\n",
    "      origin_score = 100 * roc_auc_score(test_y, test_out)\n",
    "      scores = []\n",
    "      for seed in [0]:\n",
    "          score = target3_load_dataset(dataset,seed,k,samples,train_new_model, K, overlap, mode, P)\n",
    "          scores.append(score)\n",
    "      average_score = np.mean(scores)\n",
    "      print('LUNAR : Score: %.4f \\t' %(origin_score))\n",
    "      print('Subset : Score: %.4f \\t' %(average_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRnHUviiVntc"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    k = 100\n",
    "    train_new_model = True\n",
    "    samples = 'MIXED'\n",
    "\n",
    "# List of datasets\n",
    "datasets = [\"HRSS\", \"MI-F\", \"MI-V\", \"OPTDIGITS\", \"PENDIGITS\", \"SATELLITE\", \"SHUTTLE\", \"THYROID\"]\n",
    "modes = [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyQXhQWXVbON",
    "outputId": "29b3e774-c141-43eb-96b5-5f273f7163d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: HRSS ------------------------------------------------------------------------------------------\n",
      "Percentage = 50 : \n",
      "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 82.4895 \t Runtime: 32.14 seconds\n",
      "Percentage = 70 : \n",
      "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 86.6586 \t Runtime: 44.72 seconds\n",
      "Percentage = 90 : \n",
      "Dataset: HRSS \t Samples: MIXED \t k: 100 \t Score: 90.3914 \t Runtime: 61.81 seconds\n",
      "Dataset: MI-F ------------------------------------------------------------------------------------------\n",
      "Percentage = 50 : \n",
      "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 84.1106 \t Runtime: 10.26 seconds\n",
      "Percentage = 70 : \n",
      "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 82.2215 \t Runtime: 13.43 seconds\n",
      "Percentage = 90 : \n",
      "Dataset: MI-F \t Samples: MIXED \t k: 100 \t Score: 84.1632 \t Runtime: 17.63 seconds\n",
      "Dataset: MI-V ------------------------------------------------------------------------------------------\n",
      "Percentage = 50 : \n",
      "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 95.3318 \t Runtime: 8.67 seconds\n",
      "Percentage = 70 : \n",
      "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 95.6101 \t Runtime: 11.56 seconds\n",
      "Percentage = 90 : \n",
      "Dataset: MI-V \t Samples: MIXED \t k: 100 \t Score: 96.4087 \t Runtime: 15.05 seconds\n",
      "Dataset: OPTDIGITS ------------------------------------------------------------------------------------------\n",
      "Percentage = 50 : \n",
      "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.3911 \t Runtime: 3.20 seconds\n",
      "Percentage = 70 : \n",
      "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.4356 \t Runtime: 3.81 seconds\n",
      "Percentage = 90 : \n",
      "Dataset: OPTDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.4400 \t Runtime: 4.09 seconds\n",
      "Dataset: PENDIGITS ------------------------------------------------------------------------------------------\n",
      "Percentage = 50 : \n",
      "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.8439 \t Runtime: 2.14 seconds\n",
      "Percentage = 70 : \n",
      "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.8397 \t Runtime: 4.59 seconds\n",
      "Percentage = 90 : \n",
      "Dataset: PENDIGITS \t Samples: MIXED \t k: 100 \t Score: 99.8315 \t Runtime: 3.72 seconds\n",
      "Dataset: SATELLITE ------------------------------------------------------------------------------------------\n",
      "Percentage = 50 : \n",
      "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 84.1433 \t Runtime: 2.37 seconds\n",
      "Percentage = 70 : \n",
      "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 84.6063 \t Runtime: 3.47 seconds\n",
      "Percentage = 90 : \n",
      "Dataset: SATELLITE \t Samples: MIXED \t k: 100 \t Score: 86.0120 \t Runtime: 3.81 seconds\n",
      "Dataset: SHUTTLE ------------------------------------------------------------------------------------------\n",
      "Percentage = 50 : \n",
      "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 99.8552 \t Runtime: 11.67 seconds\n",
      "Percentage = 70 : \n",
      "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 99.9651 \t Runtime: 19.32 seconds\n",
      "Percentage = 90 : \n",
      "Dataset: SHUTTLE \t Samples: MIXED \t k: 100 \t Score: 99.9390 \t Runtime: 26.62 seconds\n",
      "Dataset: THYROID ------------------------------------------------------------------------------------------\n",
      "Percentage = 50 : \n",
      "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 87.9301 \t Runtime: 2.14 seconds\n",
      "Percentage = 70 : \n",
      "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 85.2263 \t Runtime: 2.89 seconds\n",
      "Percentage = 90 : \n",
      "Dataset: THYROID \t Samples: MIXED \t k: 100 \t Score: 83.6816 \t Runtime: 3.61 seconds\n"
     ]
    }
   ],
   "source": [
    "# Run the main function\n",
    "# Loop over datasets\n",
    "for mode in modes:\n",
    "  print(f\"Mode {mode}:\")\n",
    "  for dataset in datasets:\n",
    "      print(f\"Dataset: {dataset} ------------------------------------------------------------------------------------------\")\n",
    "      args = Args()\n",
    "      args.dataset = dataset\n",
    "      args.samples = 'MIXED'\n",
    "      args.mode = mode\n",
    "      main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEC_5IQC3DPw"
   },
   "source": [
    "# Result\n",
    "![](https://drive.google.com/uc?export=view&id=1sXIuEdjwAXp7_PYvn-zE0Y11utpqToaF)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "624ffRjLX2ZY",
    "FX6awK7DYUYO",
    "87QEmASwYmUT",
    "XaD6MtKrZYYe",
    "KIePvIj5Zmwk",
    "2l9GxNi80-3y",
    "ObEhMJiA2V2f",
    "3LmyQQFJyk3_"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
